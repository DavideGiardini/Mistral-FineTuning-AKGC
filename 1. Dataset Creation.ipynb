{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Opibax6LIwm"
      },
      "source": [
        "# Data Creation\n",
        "In this first notebook we are going to:<br>\n",
        "1. Extract subgraphs from the 6 Neo4j databases\n",
        "2. Transform the subgraphs in text form\n",
        "3. Generate the texts based on the triples\n",
        "4. Format triples and texts in order to make them usable for finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV8LAh24LIwp"
      },
      "source": [
        "## Neo4j Extractor\n",
        "This is the first notebook, used for extracting triples from the 6 neo4j databases. The triples are going to be extracted by selecting a rendom node, and then identifying a subgraph with random maxLevel and limit starting from that node.<br>\n",
        "Since the database has to be stopped and restarted manually, this notebook was run for each of the 6 databases by changing the db_name and the number of samples to extract from each of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDiXnoVPLIwq"
      },
      "outputs": [],
      "source": [
        "# Import the libraries\n",
        "from neo4j import GraphDatabase\n",
        "import random\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "## Define the name of the database and the number of subgraph to extract from it\n",
        "db_name = \"graph-data-science\"\n",
        "num_samples = 200\n",
        "\n",
        "# Neo4j connection details\n",
        "uri = \"bolt://localhost:7687\"  # Update with your Neo4j URI\n",
        "user = \"neo4j\"  # Update with your Neo4j username\n",
        "password = \"*****\"  # Update with your Neo4j password\n",
        "\n",
        "# Connect to Neo4j\n",
        "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "\n",
        "# Function to extract all node ids inside the database\n",
        "def get_all_node_ids(tx):\n",
        "    result = tx.run(\"MATCH (n) RETURN ID(n) AS id\")\n",
        "    return [record[\"id\"] for record in result]\n",
        "\n",
        "# Step 1: Retrieve all node IDs\n",
        "with driver.session() as session:\n",
        "    all_node_ids = session.execute_read(get_all_node_ids)\n",
        "\n",
        "# Step 2: Randomly select node IDs\n",
        "sampled_node_ids = random.sample(all_node_ids, num_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXJPO0HvLIws"
      },
      "outputs": [],
      "source": [
        "# The coalesceDiz is used for specifying the properties of the nodes that have to be extracted from the database. For each node, the first property of the list that is not null in the node is going to be extracted.\n",
        "coalesceDiz = {\n",
        "    \"recommendations\": [\"name\", \"title\"],\n",
        "    \"graph-data-science\": [\"name\"],\n",
        "    \"legis-graph\": [\"wikipediaID\", \"name\", \"title\", \"code\", \"billID\", \"type\"],\n",
        "    \"wwc2019\": [\"name\"],\n",
        "    \"twitch\": [\"name\"],\n",
        "    \"recipes\": [\"name\"],\n",
        "    \"listings\": [\"name\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ro515avLIws",
        "outputId": "b297a6c4-9430-431c-f88b-23f5a9275860"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MATCH (n)\n",
            "WHERE ID(n) = $node_id\n",
            "CALL apoc.path.subgraphAll(n, {\n",
            "    maxLevel: $maxLevel, limit: $limit, bfs: False\n",
            "}) YIELD nodes, relationships\n",
            "UNWIND relationships AS r\n",
            "WITH r, startNode(r) AS startNode, endNode(r) AS endNode\n",
            "RETURN \n",
            "    coalesce(startNode.name, 'unknown') AS startNodeLabel, \n",
            "    type(r) AS relationshipType,\n",
            "    coalesce(endNode.name, 'unknown') AS endNodeLabel\n"
          ]
        }
      ],
      "source": [
        "# Define the query used to extract the subgraph from the database\n",
        "# The query has 3 parameters:\n",
        "# 1. The node id that we extracted before from which to start\n",
        "# 2. The maxLevel, i.e. the depth of the subgraph\n",
        "# 3. Limit, i.e. the maximum number of nodes\n",
        "query = \"\"\"MATCH (n)\n",
        "WHERE ID(n) = $node_id\n",
        "CALL apoc.path.subgraphAll(n, {\n",
        "    maxLevel: $maxLevel, limit: $limit, bfs: False\n",
        "}) YIELD nodes, relationships\n",
        "UNWIND relationships AS r\n",
        "WITH r, startNode(r) AS startNode, endNode(r) AS endNode\n",
        "RETURN\n",
        "    coalesce(\"\"\"\n",
        "\n",
        "# Insert the corresponding coalesce information in the query from the coalesceDiz\n",
        "for entity in coalesceDiz[db_name]:\n",
        "    query += \"startNode.\" + entity + \",\"\n",
        "query += \"\"\" 'unknown') AS startNodeLabel,\n",
        "    type(r) AS relationshipType,\n",
        "    coalesce(\"\"\"\n",
        "for entity in coalesceDiz[db_name]:\n",
        "    query += \"endNode.\" + entity + \",\"\n",
        "query += \" 'unknown') AS endNodeLabel\"\n",
        "\n",
        "# Final query\n",
        "print(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y0KJjCfLIwt",
        "outputId": "566d5f68-08fa-405d-f54c-7339b08b6add"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [06:51<00:00,  2.06s/it]\n"
          ]
        }
      ],
      "source": [
        "# For some movies in the recommendations dataset I noticed that the article was moved to the end (ex: \"Gentlmen The\")\n",
        "# Since this could be a problem when generating a text that contains this strange titles, we are going to solve this with regex\n",
        "def move_article_to_beginning(title: str) -> str:\n",
        "    # Use regex to match the pattern and rearrange the string\n",
        "    result = re.sub(r'^(.*), The$', r'The \\1', title)\n",
        "    return result\n",
        "\n",
        "# Function to execute the query and extract the subgraph\n",
        "def get_subgraph(node_id, maxLevel, limit):\n",
        "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "    try:\n",
        "        with driver.session() as session:\n",
        "            result = session.run(query, node_id=node_id, maxLevel=maxLevel, limit=limit)\n",
        "            return result.values()\n",
        "    finally:\n",
        "        driver.close()\n",
        "\n",
        "# Transform the extracted subgraph in text form\n",
        "def build_prompt(subgraph):\n",
        "    prompt = \"\"\n",
        "    for triple in subgraph:\n",
        "        triple[0] = move_article_to_beginning(triple[0]).strip()\n",
        "        triple[2] = move_article_to_beginning(triple[2]).strip()\n",
        "        prompt += \"(\" + re.sub(\"[(].*[)]\",\"\",triple[0]) + \") - [\" + triple[1] + \"] -> (\" + re.sub(\"[(].*[)]\",\"\",triple[2]) + \")\\n\"\n",
        "    return prompt.strip()\n",
        "\n",
        "# Run the pipeline\n",
        "for id in tqdm(sampled_node_ids):\n",
        "    # Choose a random maxLevel and limit\n",
        "    maxLevel = random.randrange(2,6)\n",
        "    limit = random.randrange(6,13)\n",
        "    # Extract subgraph by running the query\n",
        "    subG = get_subgraph(id, maxLevel, limit)\n",
        "    # Transform the extracted subgraph in text form\n",
        "    prompt = build_prompt(subG)\n",
        "    # Write to file\n",
        "    f = open(\"C:/Users/david/Desktop/NLP/data/triples/\" + db_name + \"/\" + str(id) + \".txt\", \"a\")\n",
        "    f.write(prompt)\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXmt77brLIwu"
      },
      "source": [
        "## Text Generation\n",
        "Now that the triples are extract we generate, starting from those triples, the texts.<br>\n",
        "To do so, we are going to use the gemini-pro API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-ktSpIQLIwu",
        "outputId": "5dfa4dca-c15c-4052-c24a-6b402d981562"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\david\\anaconda3\\envs\\NLPenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import re\n",
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vihzWfMGLIwv"
      },
      "outputs": [],
      "source": [
        "genai.configure(api_key=\"***\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C07wLDfeLIwv"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bbb0QKCSLIwv"
      },
      "outputs": [],
      "source": [
        "input_path = \"C:/Users/david/Desktop/NLP/data/triples/\"\n",
        "output_path = \"C:/Users/david/Desktop/NLP/data/texts/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC54gQriLIwv"
      },
      "outputs": [],
      "source": [
        "# This dictionary contains, for each database, the explanation of all of its relationships.\n",
        "# The explanations of the relationships present between the triples are going to be inserted inside the prompt in order to supply to the LLM a better understanding of them and hopefully generating better text.\n",
        "triplets_instructions = {\n",
        "    \"recommendations\": {\n",
        "        \"IN_GENRE\": \"(Movie)-[IN_GENRE]->(Genre)\",\n",
        "        \"ACTED_IN\": \"(Person/Director/Actor)-[ACTED_IN]->(Movie)\",\n",
        "        \"DIRECTED\": \"(Person/Director/Actor)-[DIRECTED]->(Movie)\",\n",
        "        \"RATED\": \"(User)-[RATED]->(Movie)\"\n",
        "    },\n",
        "    \"graph-data-science\": {\n",
        "        \"ATTACKER\": \"(House)-[ATTACKER, DEFENDER]->(Battle)\",\n",
        "        \"DEFENDER\": \"(House)-[ATTACKER, DEFENDER]->(Battle)\",\n",
        "        \"IS_IN\": \"(Battle, Location)-[IS_IN]->(Location, Region)\",\n",
        "        \"DEFENDER_COMMANDER\": \"(Knight, King)-[DEFENDER_COMMANDER,ATTACKER_COMMANDER]->(Battle)\",\n",
        "        \"ATTACKER_COMMANDER\": \"(Knight, King)-[DEFENDER_COMMANDER,ATTACKER_COMMANDER]->(Battle)\",\n",
        "        \"DEFENDER_KING\": \"(King)-[DEFENDER_KING, ATTACKER_KING]->(Battle)\",\n",
        "        \"ATTACKER_KING\": \"(King)-[DEFENDER_KING, ATTACKER_KING]->(Battle)\",\n",
        "        \"BELONGS_TO\": \"(Person,Knight, King)-[BELONGS_TO]->(House)\",\n",
        "        \"HAS_STATUS\": \"(Person,Knight, King)-[HAS_STATUS]->(Status)\",\n",
        "        \"APPEARED_IN\": \"(Person,Knight,King)-[APPEARED_IN, DIED_IN]->(Book)\",\n",
        "        \"DIED_IN\": \"(Person,Knight,King)-[APPEARED_IN, DIED_IN]->(Book)\",\n",
        "        \"MEMBER_OF_CULTURE\": \"(Person, King, Knight)-[MEMBER_OF_CULTURE]->(Culture)\",\n",
        "        \"INTERACTS\": \"(Person, Knight, King)-[INTERACTS]->(Person, Knight, King)\",\n",
        "        \"RELATED_TO\": \"(Person, Knight, King)<-[RELATED_TO]->(Person, Knight, King)\"\n",
        "    },\n",
        "    \"legis-graph\": {\n",
        "        \"REPRESENTS\": \"(Legislator)-[REPRESENTS]->(StateCode)\",\n",
        "        \"IS_MEMBER_OF\": \"(Legislator)-[IS_MEMBER_OF]->(Party)\",\n",
        "        \"ELECTED_TO\": \"(Legislator)-[ELECTED_TO]->(Body)\",\n",
        "        \"SPONSORED_BY\": \"(BillId)-[SPONSORED_BY]->(Legislator)\",\n",
        "        \"VOTED_ON\": \"(Legislator)-[VOTED_ON]->(BillId)\",\n",
        "        \"REFERRED_TO\": \"(BillId)-[REFERRED_TO]->(Committee)\",\n",
        "        \"SERVES_ON\": \"(Legislator)-[SERVES_ON]->(Committee)\",\n",
        "        \"DEALS_WITH\": \"(BillId)-[DEALS_WITH]->(Subject)\"\n",
        "    },\n",
        "    \"wwc2019\": {\n",
        "        \"PARTICIPATED_IN\": \"(Team)-[PARTICIPATED_IN]->(Tournament)\",\n",
        "        \"PLAYED_IN\": \"(Person)-[PLAYED_IN]->(Tournament)\",\n",
        "        \"REPRESENTS\": \"(Person)-[REPRESENTS]->(Team)\"\n",
        "    },\n",
        "    \"recipes\": {\n",
        "        \"WROTE\": \"(Author)-[WROTE]->(Recipe)\",\n",
        "        \"CONTAINS_INGREDIENT\": \"(Recipe)-[CONTAINS_INGREDIENT]->(Ingredient)\",\n",
        "        \"DIET_TYPE\": \"(Recipe)-[DIET_TYPE]->(DietType)\",\n",
        "        \"COLLECTION\": \"(Recipe)-[COLLECTION]->(Collection)\"\n",
        "    },\n",
        "    \"listings\": {\n",
        "        \"IN_NEIGHBORHOOD\": \"(ListingTitle)-[IN_NEIGHBORHOOD]->(Neighborhood)\",\n",
        "        \"HAS\": \"(ListingTitle)-[HAS]->(Amenity)\",\n",
        "        \"HOSTS\": \"(Host)-[HOSTS]->(ListingTitle)\",\n",
        "        \"REVIEWED\": \"(User)-[REVIEWED]->(ListingTitle)\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# This is the base system prompt\n",
        "base_system_prompt = \"\"\"\n",
        "Imagine being a text generator from Knowledge Graphs.\n",
        "Based on the triples provided in the context, generate a short text containing all the information contained in the triples.\n",
        "Make sure not to add any information of the entities mentioned in the triples that is not coming from the knowledge graph.\n",
        "Even though the usage of pronouns is allowed, make sure not to modify the names of the entities.\n",
        "The text you generate should not be a simple mention of all the facts stored in the triples, but you should write them in an original way.\n",
        "The text should resemble a \"\"\"\n",
        "\n",
        "# The style of the prompt is going to be randomly picked from this list\n",
        "styles = [\"blog article.\", \"wikipedia article.\", \"newspaper article.\", \"reddit post.\", \"YouTube script.\", \"podcast transcript.\"]\n",
        "\n",
        "# These are specific instructions for some dbs\n",
        "db_specific_instructions = {\n",
        "    \"recipes\": \"Insert the recipes' titles inside quotes.\",\n",
        "    \"listings\": \"Insert the listings' titles inside quotes.\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6xN-woBLIww",
        "outputId": "728bf2a2-9aec-4346-964d-1eff628b2281"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [27:26<00:00,  3.29s/it]\n",
            "100%|██████████| 455/455 [24:49<00:00,  3.27s/it]\n",
            "100%|██████████| 450/450 [29:14<00:00,  3.90s/it]\n",
            "100%|██████████| 450/450 [25:52<00:00,  3.45s/it]\n"
          ]
        }
      ],
      "source": [
        "# Function to generate the prompt\n",
        "def generateSystemPrompt(rel_types):\n",
        "    # Pick a random style from the list\n",
        "    style_num = random.randrange(0,6)\n",
        "    # Attach the random style to the base system prompt\n",
        "    system_prompt = base_system_prompt + styles[style_num]\n",
        "    # Add db specific instruction if needed\n",
        "    if db_name in db_specific_instructions.keys():\n",
        "        system_prompt += \"\\n\" + db_specific_instructions[db_name]\n",
        "    # Add db structure\n",
        "    system_prompt += \"\\n\\nThis is the KB structure:\"\n",
        "    triplets_lst = []\n",
        "    for el in rel_types:\n",
        "        triplets_lst.append(triplets_instructions[db_name][el])\n",
        "    triplets_lst = list(set(triplets_lst))\n",
        "    for el in triplets_lst:\n",
        "        system_prompt += \"\\n\" + el\n",
        "    system_prompt += \"\\n\\n Context:\"\n",
        "    return system_prompt\n",
        "\n",
        "# Run the code for each of the db\n",
        "# (This function was later run also for the other 2)\n",
        "for db_name in [\"recommendations\", \"legis-graph\", \"recipes\", \"listings\"]:\n",
        "    file_lst = os.listdir(input_path + db_name)\n",
        "    file_lst = list(set(file_lst).difference(os.listdir(output_path + db_name)))\n",
        "    # Loop throught the triples files\n",
        "    for file_name in tqdm(file_lst):\n",
        "        # Read file\n",
        "        file_path = db_name + \"/\" + file_name\n",
        "        file = open(input_path + file_path, \"r\")\n",
        "        triples = file.read()\n",
        "        # Extract the relationships types in the file\n",
        "        rel_types = list(set(re.findall(r\"\\[(.*)\\]\", triples)))\n",
        "        # Generate prompt\n",
        "        system_prompt = generateSystemPrompt(rel_types)\n",
        "        # Concatenate context triples\n",
        "        final_prompt = system_prompt + '\\n\\n' + triples\n",
        "        # Inference with Gemini Pro\n",
        "        try:\n",
        "            response = model.generate_content(final_prompt).text\n",
        "        except:\n",
        "            continue\n",
        "        # Write the file to the output_path\n",
        "        f = open(output_path + file_path, \"a\")\n",
        "        f.write(response)\n",
        "        f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5vgGYjQLIww"
      },
      "source": [
        "## Formatter\n",
        "Now that we have both the triples and the text, we are going to format it in order to make it usable for finetuning the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_8BTpNlLIww"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCRoZe5zLIwx"
      },
      "outputs": [],
      "source": [
        "# The schema dictionary is used to insert inside the prompt the schema of the corresponding database, i.e. the schema that the FineTuned LLM will have to follow in order to extract the triples form the text.\n",
        "schema = {\n",
        "    \"recommendations\": \"\"\"(Movie)-[IN_GENRE]->(Genre)\n",
        "(Person/Director/Actor)-[ACTED_IN]->(Movie)\n",
        "(Person/Director/Actor)-[DIRECTED]->(Movie)\n",
        "(User)-[RATED]->(Movie)\"\"\",\n",
        "    \"graph-data-science\": \"\"\"(House)-[ATTACKER, DEFENDER]->(Battle)\n",
        "(House)-[ATTACKER, DEFENDER]->(Battle)\n",
        "(Battle, Location)-[IS_IN]->(Location, Region)\n",
        "(Knight, King)-[DEFENDER_COMMANDER,ATTACKER_COMMANDER]->(Battle)\n",
        "(Knight, King)-[DEFENDER_COMMANDER,ATTACKER_COMMANDER]->(Battle)\n",
        "(King)-[DEFENDER_KING, ATTACKER_KING]->(Battle)\n",
        "(King)-[DEFENDER_KING, ATTACKER_KING]->(Battle)\n",
        "(Person,Knight, King)-[BELONGS_TO]->(House)\n",
        "(Person,Knight, King)-[HAS_STATUS]->(Status)\n",
        "(Person,Knight,King)-[APPEARED_IN, DIED_IN]->(Book)\n",
        "(Person,Knight,King)-[APPEARED_IN, DIED_IN]->(Book)\n",
        "(Person, King, Knight)-[MEMBER_OF_CULTURE]->(Culture)\n",
        "(Person, Knight, King)-[INTERACTS]->(Person, Knight, King)\n",
        "(Person, Knight, King)<-[RELATED_TO]->(Person, Knight, King)\"\"\",\n",
        "    \"legis-graph\": \"\"\"(Legislator)-[REPRESENTS]->(StateCode)\n",
        "(Legislator)-[IS_MEMBER_OF]->(Party)\n",
        "(Legislator)-[ELECTED_TO]->(Body)\n",
        "(BillId)-[SPONSORED_BY]->(Legislator)\n",
        "(Legislator)-[VOTED_ON]->(BillId)\n",
        "(BillId)-[REFERRED_TO]->(Committee)\n",
        "(Legislator)-[SERVES_ON]->(Committee)\n",
        "(BillId)-[DEALS_WITH]->(Subject)\"\"\",\n",
        "    \"wwc2019\": \"\"\"(Team)-[PARTICIPATED_IN]->(Tournament)\n",
        "(Person)-[PLAYED_IN]->(Tournament)\n",
        "(Person)-[REPRESENTS]->(Team)\"\"\",\n",
        "    \"recipes\": \"\"\"(Author)-[WROTE]->(Recipe)\n",
        "(Recipe)-[CONTAINS_INGREDIENT]->(Ingredient)\n",
        "(Recipe)-[DIET_TYPE]->(DietType)\n",
        "(Recipe)-[COLLECTION]->(Collection)\"\"\",\n",
        "    \"listings\": \"\"\"(ListingTitle)-[IN_NEIGHBORHOOD]->(Neighborhood)\n",
        "(ListingTitle)-[HAS]->(Amenity)\n",
        "(Host)-[HOSTS]->(ListingTitle)\n",
        "(User)-[REVIEWED]->(ListingTitle)\"\"\"\n",
        "}\n",
        "\n",
        "base_prompt = \"\"\"<s>[INST]Imagine being a Knowledge Graph constructor from unstructured text.\n",
        "Following the schema provided, extract all the triples you can find in the text.\n",
        "\n",
        "Schema:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afoitRILLIwx"
      },
      "outputs": [],
      "source": [
        "triples_path = \"C:/Users/david/Desktop/NLP/data/triples/\"\n",
        "text_path = \"C:/Users/david/Desktop/NLP/data/texts/\"\n",
        "output_path = \"C:/Users/david/Desktop/NLP/data/train4/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoKZJPKKLIwx",
        "outputId": "3e2d0739-f288-4e54-9185-05d02bd44def"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:09<00:00, 105.27it/s]\n",
            "100%|██████████| 995/995 [00:09<00:00, 107.45it/s]\n",
            "100%|██████████| 992/992 [00:09<00:00, 104.64it/s]\n",
            "100%|██████████| 986/986 [00:09<00:00, 102.86it/s]\n"
          ]
        }
      ],
      "source": [
        "# For each of the four databases used for training\n",
        "for db_name in os.listdir(output_path):\n",
        "    # Concatenate the base prompt with the schema of the relevant db\n",
        "    db_prompt = base_prompt + schema[db_name] + '\\n\\nContext:\\n\"'\n",
        "    # For each of the triple/text file:\n",
        "    for file_name in tqdm(os.listdir(text_path + db_name)):\n",
        "        # Read text file\n",
        "        text = open(text_path + db_name + '/' + file_name, \"r\").read()\n",
        "        text = \"\\n\".join(text.split(\"\\n\\n\"))\n",
        "        # Append text file to prompt\n",
        "        final_prompt = db_prompt + text + '\"[/INST]\\n\\nExtracted Triples:\\n'\n",
        "        # Read triple file\n",
        "        try:\n",
        "            triples = open(triples_path + db_name + '/' + file_name, \"r\").read()\n",
        "        except:\n",
        "            print(\"Could not find triples of \" + db_name + '/' + file_name)\n",
        "            continue\n",
        "        # Append triples to prompt\n",
        "        final_prompt += triples\n",
        "        final_prompt += \"</s>\"\n",
        "        # Write the prompt into the output path\n",
        "        f = open(output_path + db_name + '/' + file_name, \"w\")\n",
        "        f.write(final_prompt)\n",
        "        f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RGXVlmmLIwy"
      },
      "source": [
        "Now that everything is saved, we open the files and put them into our datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaJc2g_tLIwy"
      },
      "outputs": [],
      "source": [
        "# Insert everything into a list\n",
        "lst = []\n",
        "for db_name in os.listdir(output_path):\n",
        "    if db_name == 'datasets':\n",
        "        continue\n",
        "    listdir = os.listdir(output_path + db_name)\n",
        "    for file in listdir:\n",
        "        text = open(output_path + db_name + '/' + file, \"r\").read()\n",
        "        lst.append(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJBQdkzZLIwz"
      },
      "outputs": [],
      "source": [
        "# Shuffle the list\n",
        "random.shuffle(lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTZEYjO9LIwz"
      },
      "outputs": [],
      "source": [
        "# Divide in train, validation, and test (2400, 400, 400)\n",
        "# Note that we can slice the list in this way, and not randomly, because we shuffled it previously\n",
        "df_train = pd.DataFrame(lst[:2400])\n",
        "df_val = pd.DataFrame(lst[2400:2800])\n",
        "df_test = pd.DataFrame(lst[-400:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2I2q-y2LIwz"
      },
      "outputs": [],
      "source": [
        "# Save train and validation into parquet format (and manually upload to huggingFace: Giardooo/KG_constructor)\n",
        "table = pa.Table.from_pandas(df_train)\n",
        "pq.write_table(table, 'train-00000-of-00001.parquet')\n",
        "table = pa.Table.from_pandas(df_val)\n",
        "pq.write_table(table, 'test-00000-of-00001.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hk4NdSuLIw0"
      },
      "outputs": [],
      "source": [
        "# Save test file to csv\n",
        "df_test.to_csv(\"test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3NVO_LQLIw0"
      },
      "source": [
        "The same pipeline was applied for the two external databases, resulting in a 210 observation datasets saved as \"test_external_dbs.csv\".<br>\n",
        "The observations are 210 because 10 of them (5 for each database) will be used as examples in the few shot prompting technique, while the other 200 (100 per database) for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation for Few Shot Finetuning\n",
        "In this section we are going to create the dataset that is going to be used to finetune a model over prompts that contain examples.<br>\n",
        "We are going to do this by importing the previous dataset, and inserting in each of the observation three other random observations coming from the same dataset and related to the same KG structure as the text from which the LLM will have to extract the triples.<br>\n",
        "We used three examples rather than five to keep the observations shorter. In fact, when trying to fine-tune the LLM with prompts containing 5 examples, we stumbled upon OOM errors."
      ],
      "metadata": {
        "id": "nmngAH1ULMni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries and data"
      ],
      "metadata": {
        "id": "1E_leQEdVJex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random as rd"
      ],
      "metadata": {
        "id": "FIV2YWNySck-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315,
          "referenced_widgets": [
            "29bc2c7c7dc94050ae5f0cf39da5116c",
            "98a491fdc40a477e930db577be33c69c",
            "dc8e908d89254b05aac052ef1a9533c4",
            "aeedb92f85d04b4ea08ec23f0c49e626",
            "f3e5176347f54cc390fa7ffa6e8310d6",
            "1fa68fa646df41d99a975487b3a0a21d",
            "779fa0e6013b40fa9c15a082b7a1e98b",
            "fd357d013e594b729c82019c64ee76a9",
            "49f084bb804f4d75acb2895a940d8206",
            "439487a5095749f4941a70c63fb87b05",
            "0bdfc39d47af4e72a5bcf3a4097b8cf4",
            "c5062abe65534120833842fc9c3155f4",
            "ba526027c317473793da9203828b5f14",
            "e8b503426cb6465a898b5638849fd57a",
            "55e0cf300913437ea20d5e9f44bc5cdf",
            "cb21fe9430824f5dbf3c970a8726dc01",
            "4a9f23bf4b8c4b8b96c6301ae90c924c",
            "8caddffe1725456c8eb873b6b9fab57a",
            "1e68ecf2dc264713a7ed998b6466ce30",
            "5ebe358df088413c9eb1ff51cb9cd657",
            "f2a3c5efca3241b5a4419d7046ad9eac",
            "ac9da6413cdd498ab6ce90c19ec9fc2a",
            "d93e41c06616441b823b00ae68602c30",
            "e3f1b5dd61554a20887f25f6dd7ab1bc",
            "29d1cc45792d4030a56fff6caa9f3fc8",
            "c6bcb055ab024045aed6cbffb7ddbcbc",
            "1b6f7c74de4747ff89b696ef496c37b7",
            "611b8a119e8248b0837b9b8f812045a7",
            "b9920c1e98684cbdb27166802d6d083c",
            "7d4dec73188745e69b4336b6a0b80e1f",
            "766a6b1790e640708c057d6c0e33fc2a",
            "a574f547074d41bd945bc73600bc7064",
            "df9cea1c9620417ea503fcde249910c5",
            "5de735b96e0d4ba8921d6e0ecee791e0",
            "c83ef0b8b26a4ab5aa78dcac4a7b20e8",
            "370cc55a1f5c4ee7803a7fa81acdd0f7",
            "102d037f2c934ef6b603058ea5116797",
            "c484ce70b57843e78cb8c5f3fe328034",
            "d94769539d8e4f44bce469ba5f0fbd91",
            "b9bed8b37e964f96bf79b4e6ce4d63a8",
            "c44c31b09465400088e4066e6ba2520c",
            "3cab37b7aeec4f50b5d5132885c3ec98",
            "7f062af035fb4b55952d901056bef5b9",
            "71c7da7525284f64bbe521aa0d01ddbc"
          ]
        },
        "id": "j_hUm0rwPpo-",
        "outputId": "ed1420f4-51ff-4b0b-9703-913b0ee3aea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.57M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29bc2c7c7dc94050ae5f0cf39da5116c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/276k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5062abe65534120833842fc9c3155f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/2400 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d93e41c06616441b823b00ae68602c30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/400 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5de735b96e0d4ba8921d6e0ecee791e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['0'],\n",
            "        num_rows: 2400\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['0'],\n",
            "        num_rows: 400\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "# Load Dataset\n",
        "data = load_dataset(\"Giardooo/KG_constructor\")\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classify Data\n",
        "We build a classifier function that helps us classify the observations back into the datasets they were extracted from.<br>"
      ],
      "metadata": {
        "id": "FShglZmGVMww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classifier(text):\n",
        "  # The element are classified based on their univoque relatinoship types\n",
        "  if \"DEALS_WITH\" in text:\n",
        "    return \"legis_graph\"\n",
        "  elif \"ACTED_IN\" in text:\n",
        "    return \"recommendations\"\n",
        "  elif \"COLLECTION\" in text:\n",
        "    return \"recipes\"\n",
        "  elif \"IN_NEIGHBORHOOD\" in text:\n",
        "    return \"listings\"\n",
        "  else:\n",
        "    print(text)"
      ],
      "metadata": {
        "id": "mmSJTrpzW_X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize class_diz\n",
        "# In this dictionary we are going to store, both for train and validation, the index of the observations coming from each of the original datasets.\n",
        "class_diz = {\"train\": {\"legis_graph\": [], \"recommendations\": [], \"recipes\": [], \"listings\": []}, \"val\": {\"legis_graph\": [], \"recommendations\": [], \"recipes\": [], \"listings\": []}}\n",
        "\n",
        "for i in range(len(data[\"train\"][\"0\"])):\n",
        "  el = data[\"train\"][\"0\"][i]\n",
        "  # Classify element\n",
        "  classification =  classifier(el)\n",
        "  class_diz[\"train\"][classification].append(i)\n",
        "\n",
        "for i in range(len(data[\"test\"][\"0\"])):\n",
        "  el = data[\"test\"][\"0\"][i]\n",
        "  # Classify element\n",
        "  classification =  classifier(el)\n",
        "  class_diz[\"val\"][classification].append(i)"
      ],
      "metadata": {
        "id": "P71L1JeZX2O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Examples"
      ],
      "metadata": {
        "id": "BvOYj7FaWflE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The function format_examples takes in input a list of 3 indexes and the data from which to extract the observations\n",
        "# It then formats the three observations coming from this 3 indexes in a way that is suitable for the prompt\n",
        "def format_examples(extract, data):\n",
        "  example_txt = \"\"\n",
        "  for el in extract:\n",
        "    example_txt += \"\\nContext:\\n\"\n",
        "    example_txt += data[el].split(\"Context:\\n\")[-1]\n",
        "    example_txt += \"\\n-------------\"\n",
        "  example_txt = example_txt.replace(\"[/INST]\", \"\").replace(\"</s>\", \"\")\n",
        "  return example_txt\n",
        "\n",
        "# The function select_examples takes in input the position of the observation that we want to augment with examples and the data from which to extract the observations\n",
        "# It then uses the class_diz to extract three examples coming from the same original dataset of the observation\n",
        "def select_examples(position, data, train_val):\n",
        "  # Classify current element\n",
        "  classification =  classifier(data[position])\n",
        "  # Get list of text of the same class\n",
        "  lst = class_diz[train_val][classification].copy()\n",
        "  # Remove current element\n",
        "  lst.remove(position)\n",
        "  extract = rd.sample(lst, 3)\n",
        "  # Now that we have our list, format the 3 examples\n",
        "  return format_examples(extract, data)"
      ],
      "metadata": {
        "id": "mozUYYXtQReu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the new datasets\n",
        "train_lst = []\n",
        "val_lst = []\n",
        "\n",
        "for i in range(len(data[\"train\"][\"0\"])):\n",
        "  obs = data[\"train\"][\"0\"][i]\n",
        "  new_text = obs.split(\"Context:\\n\")[0]\n",
        "  new_text += \"Here are some examples:\"\n",
        "  new_text += select_examples(i, data[\"train\"][\"0\"], \"train\")\n",
        "  new_text += \"\\nContext\\n\"\n",
        "  new_text += obs.split(\"Context:\\n\")[-1]\n",
        "  train_lst.append(new_text)\n",
        "\n",
        "for i in range(len(data[\"test\"][\"0\"])):\n",
        "  obs = data[\"test\"][\"0\"][i]\n",
        "  new_text = obs.split(\"Context:\\n\")[0]\n",
        "  new_text += \"Here are some examples:\"\n",
        "  new_text += select_examples(i, data[\"test\"][\"0\"], \"val\")\n",
        "  new_text += \"\\nContext\\n\"\n",
        "  new_text += obs.split(\"Context:\\n\")[-1]\n",
        "  val_lst.append(new_text)"
      ],
      "metadata": {
        "id": "kEUSkQZUQqML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_lst[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yNV9j9nSv4c",
        "outputId": "6116d840-e138-49b9-9643-d4216adf7072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST]Imagine being a Knowledge Graph constructor from unstructured text.\n",
            "Following the schema provided, extract all the triples you can find in the text.\n",
            "\n",
            "Schema:\n",
            "(Legislator)-[REPRESENTS]->(StateCode)\n",
            "(Legislator)-[IS_MEMBER_OF]->(Party)\n",
            "(Legislator)-[ELECTED_TO]->(Body)\n",
            "(BillId)-[SPONSORED_BY]->(Legislator)\n",
            "(Legislator)-[VOTED_ON]->(BillId)\n",
            "(BillId)-[REFERRED_TO]->(Committee)\n",
            "(Legislator)-[SERVES_ON]->(Committee)\n",
            "(BillId)-[DEALS_WITH]->(Subject)\n",
            "\n",
            "Here are some examples:\n",
            "Context:\n",
            "\"Coming up today, we have the hot-off-the-press update on bill hr2804-114, which deals with state and local government operations. This bill has a long list of sponsors, including Mike Quigley, Zoe Lofgren, Matt Cartwright, Don Beyer, Alan Lowenthal, Marc Veasey, Jared Huffman, and Raúl Grijalva. Stay tuned for more details as they emerge!\"\n",
            "\n",
            "Extracted Triples:\n",
            "(s1400-114) - [DEALS_WITH] -> (State and local government operations)\n",
            "(hr2804-114) - [SPONSORED_BY] -> (Mike Quigley )\n",
            "(hr2804-114) - [SPONSORED_BY] -> (Zoe Lofgren)\n",
            "(hr2804-114) - [SPONSORED_BY] -> (Matt Cartwright)\n",
            "(hr2804-114) - [SPONSORED_BY] -> (Don Beyer)\n",
            "(hr2804-114) - [SPONSORED_BY] -> (Alan Lowenthal)\n",
            "(hr2804-114) - [SPONSORED_BY] -> (Marc Veasey)\n",
            "(hr2804-114) - [SPONSORED_BY] -> (Jared Huffman)\n",
            "(hr2804-114) - [SPONSORED_BY] -> (Raúl Grijalva)\n",
            "(hr2804-114) - [SPONSORED_BY] -> (Zoe Lofgren)\n",
            "(hr2804-114) - [SPONSORED_BY] -> (Mike Quigley )\n",
            "(hr2804-114) - [SPONSORED_BY] -> (Matt Cartwright)\n",
            "(hr2804-114) - [SPONSORED_BY] -> (Marc Veasey)\n",
            "(hr2804-114) - [SPONSORED_BY] -> (Raúl Grijalva)\n",
            "(hr2804-114) - [SPONSORED_BY] -> (Jared Huffman)\n",
            "(hr2804-114) - [SPONSORED_BY] -> (Alan Lowenthal)\n",
            "(hr2804-114) - [SPONSORED_BY] -> (Don Beyer)\n",
            "(hr2804-114) - [DEALS_WITH] -> (State and local government operations)\n",
            "-------------\n",
            "Context:\n",
            "\"Representative Amata Coleman Radewagen recently made headlines as the sole sponsor of three bills in the House: hr2635-114, hr4455-114, and hr5229-114. The Republican legislator hails from the American Samoa (AS) and has been elected to the House of Representatives. Besides being a member of the Republican Party, Radewagen was also recently joined in the House by fellow Californian Ken Calvert and Linda Sánchez. While Sánchez represents California, she has been elected to the House and is not affiliated with any party.\"\n",
            "\n",
            "Extracted Triples:\n",
            "(Linda Sánchez) - [REPRESENTS] -> (CA)\n",
            "(Linda Sánchez) - [ELECTED_TO] -> (House)\n",
            "(Ken Calvert) - [REPRESENTS] -> (CA)\n",
            "(Ken Calvert) - [IS_MEMBER_OF] -> (Republican)\n",
            "(Ken Calvert) - [ELECTED_TO] -> (House)\n",
            "(Amata Coleman Radewagen) - [REPRESENTS] -> (AS)\n",
            "(Amata Coleman Radewagen) - [IS_MEMBER_OF] -> (Republican)\n",
            "(Amata Coleman Radewagen) - [ELECTED_TO] -> (House)\n",
            "(hr2635-114) - [SPONSORED_BY] -> (Amata Coleman Radewagen)\n",
            "(hr2635-114) - [SPONSORED_BY] -> (Amata Coleman Radewagen)\n",
            "(hr4455-114) - [SPONSORED_BY] -> (Amata Coleman Radewagen)\n",
            "(hr4455-114) - [SPONSORED_BY] -> (Amata Coleman Radewagen)\n",
            "(hr5229-114) - [SPONSORED_BY] -> (Amata Coleman Radewagen)\n",
            "(hr5229-114) - [SPONSORED_BY] -> (Amata Coleman Radewagen)\n",
            "-------------\n",
            "Context:\n",
            "\"**Medical Research on the Rise**\n",
            "Redditors, bills s139-114, hr3729-114, hr2921-114, s2382-114, and hr1925-114 are all focused on one important topic: medical research. With so much attention being paid to this crucial field, it's clear that our future health is looking brighter than ever.\"\n",
            "\n",
            "Extracted Triples:\n",
            "(s139-114) - [DEALS_WITH] -> (Medical research)\n",
            "(hr3729-114) - [DEALS_WITH] -> (Medical research)\n",
            "(hr2921-114) - [DEALS_WITH] -> (Medical research)\n",
            "(s2382-114) - [DEALS_WITH] -> (Medical research)\n",
            "(hr1925-114) - [DEALS_WITH] -> (Medical research)\n",
            "-------------\n",
            "Context\n",
            "\"Bills like the hr5833-114, hr4036-114, hr6311-114, s1910-114, hr2864-114, hr2910-114, and s2659-114 all deal with environmental regulatory procedures.\"[/INST]\n",
            "\n",
            "Extracted Triples:\n",
            "(hr5833-114) - [DEALS_WITH] -> (Environmental regulatory procedures)\n",
            "(hr4036-114) - [DEALS_WITH] -> (Environmental regulatory procedures)\n",
            "(hr6311-114) - [DEALS_WITH] -> (Environmental regulatory procedures)\n",
            "(s1910-114) - [DEALS_WITH] -> (Environmental regulatory procedures)\n",
            "(hr2864-114) - [DEALS_WITH] -> (Environmental regulatory procedures)\n",
            "(hr2910-114) - [DEALS_WITH] -> (Environmental regulatory procedures)\n",
            "(s2659-114) - [DEALS_WITH] -> (Environmental regulatory procedures)</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The length of the new dataset corresponds to the length of the previous ones\n",
        "print(\"length of training list: \" + str(len(train_lst)))\n",
        "print(\"length of val list: \" + str(len(val_lst)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0wtomiWSyFu",
        "outputId": "cabd44d2-2a42-4e70-bcda-c279a0c58635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of training list: 2400\n",
            "length of val list: 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save to parquet\n",
        "And later upload to HuggingFace"
      ],
      "metadata": {
        "id": "DAemoYUMN1sG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "9ot1P1EZc07s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.DataFrame(train_lst)\n",
        "df_val = pd.DataFrame(val_lst)\n",
        "# Save train and validation into parquet format (and manually upload to huggingFace: Giardooo/KG_constructor_FS)\n",
        "table = pa.Table.from_pandas(df_train)\n",
        "pq.write_table(table, 'train-00000-of-00001.parquet')\n",
        "table = pa.Table.from_pandas(df_val)\n",
        "pq.write_table(table, 'test-00000-of-00001.parquet')"
      ],
      "metadata": {
        "id": "lNPLa_HnbWET"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}