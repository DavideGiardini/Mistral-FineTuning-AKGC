{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFd2XrDCLDMs"
      },
      "source": [
        "# Mistral 7B Fine-Tuning\n",
        "Now that our data is ready, we finetune Mistral 7B Instruct with QLora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZadKkWLLDMu"
      },
      "outputs": [],
      "source": [
        "!pip install matplotlib\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U datasets scipy ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoRoyB2sLDMw"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from huggingface_hub import notebook_login\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbvjuR6PLDMw"
      },
      "source": [
        "## Load Dataset and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "74bb6ebe8ff243a0909b2d463766a2af",
            "55d549c5c24949dbb443fdc1a7bfab02",
            "499ed12fd1b54f6c9c24de3cee8fd2e5",
            "c9fcd1ca34014e35882bad25cdfb7e67"
          ]
        },
        "id": "YS19wq8eLDMw",
        "outputId": "642f2225-30d4-474d-8758-afeb80a5fcbc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74bb6ebe8ff243a0909b2d463766a2af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.57M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55d549c5c24949dbb443fdc1a7bfab02",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/276k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "499ed12fd1b54f6c9c24de3cee8fd2e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/2400 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9fcd1ca34014e35882bad25cdfb7e67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/400 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['0'],\n",
            "        num_rows: 2400\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['0'],\n",
            "        num_rows: 400\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Load Dataset\n",
        "data = load_dataset(\"Giardooo/KG_constructor\")\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "426f889b15854faca9f43b9aac1d9230"
          ]
        },
        "id": "wRjfU6pDLDMx",
        "outputId": "12590ecc-bda6-4e4e-ce2b-be860f4e1cbc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "426f889b15854faca9f43b9aac1d9230",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Login to Hugginface\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "7d9d57fc32934cf5a5c7c09023ddad1b",
            "9eff95e7bece430fb7e30a7e9176deee",
            "e0e3f203b8f945c8987fcdb10283a494",
            "0a399151797448b4808c8fdb358a6816",
            "99ce9253ecf346aeb7d7b3ca39e4f700",
            "997fae2bc4534ff78b7dae542d608ff2",
            "790dc61ff54f4fe596b92d47df2c186b"
          ]
        },
        "id": "vHCHc1DcLDMx",
        "outputId": "7dd5e19f-f151-4dde-939a-b686f9fa1fe0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d9d57fc32934cf5a5c7c09023ddad1b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9eff95e7bece430fb7e30a7e9176deee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0e3f203b8f945c8987fcdb10283a494",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a399151797448b4808c8fdb358a6816",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99ce9253ecf346aeb7d7b3ca39e4f700",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "997fae2bc4534ff78b7dae542d608ff2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "790dc61ff54f4fe596b92d47df2c186b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load Model\n",
        "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    # Load the 4 bit quantized model\n",
        "    load_in_4bit=True,\n",
        "    # Use nested quantization\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    # Define the quantization data type to NF4\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    # Set the computational type to bf16 for speed-up\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nKr4iD3LDMy"
      },
      "source": [
        "## Set up the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "d82dfb7b2f1448dda4b3bb2ec7e68da8",
            "a9ebcd02fc3d4e9db4e0b36fce5ae4e9",
            "ca7d249a7e08452a9de810e96e49190e",
            "f81ca3009bf64dbda120aba992ceaa18"
          ]
        },
        "id": "4IJTxcPlLDMy",
        "outputId": "c39901f8-2bce-4690-c795-da05ee701a76"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d82dfb7b2f1448dda4b3bb2ec7e68da8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9ebcd02fc3d4e9db4e0b36fce5ae4e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca7d249a7e08452a9de810e96e49190e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f81ca3009bf64dbda120aba992ceaa18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Set up tokenizer without max tokens\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    # Set up Mistral tokenizer\n",
        "    base_model_id,\n",
        "    # Since Mistral is decoder-only, left pad is preferred\n",
        "    padding_side=\"left\",\n",
        "    # End Of Sequence token\n",
        "    add_eos_token=True)\n",
        "\n",
        "# Set pad token to unk_token\n",
        "# The default pad_token = eos_token results in the LLM forgetting the appropriate usage of the eos token, making it never stop generating text\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "\n",
        "def tokenize(examples):\n",
        "    # Extract the column of the dataset containing the text\n",
        "    text = examples[\"0\"]\n",
        "    result = tokenizer(\n",
        "        text,\n",
        "        # truncation=True,\n",
        "    )\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9b7df83acd874c91806827aba091462f",
            "3ccb86c5bf504a408a35aed7a61240e2"
          ]
        },
        "id": "-6wXIQ-bLDMy",
        "outputId": "e4c9a0eb-a4a3-465e-c1ef-71865c352edb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b7df83acd874c91806827aba091462f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ccb86c5bf504a408a35aed7a61240e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Tokenize the dataset\n",
        "tokenized_data = data.map(tokenize, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW6rFn5DLDMz"
      },
      "source": [
        "# Choose max_length\n",
        "Let's check the length (in token) of our observations. In this way we are going to choose the max_length parameter of our tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYhVpd-pLDMz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hHiz1P7LDMz",
        "outputId": "3e235021-52d4-47a4-debd-592b54d945e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2800\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def plot_data_lengths(train, val):\n",
        "    lengths = [len(x['input_ids']) for x in train]\n",
        "    lengths += [len(x['input_ids']) for x in val]\n",
        "    print(len(lengths))\n",
        "\n",
        "    # Plotting the histogram\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
        "    plt.xlabel('Num of tokens')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of Num of tokens')\n",
        "    plt.show()\n",
        "\n",
        "plot_data_lengths(tokenized_data['train'], tokenized_data['test'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e7p8V4QLDM0"
      },
      "source": [
        "1024 seems a reasonable max_length for our tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "1b36a8e256f94d0b8246c5fc83dabc9b",
            "303aa066e0c84e099efcb125ce2be2db"
          ]
        },
        "id": "ggIl8dNfLDM0",
        "outputId": "ccf0bfb1-b29a-4172-aeba-5879b998ad9e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b36a8e256f94d0b8246c5fc83dabc9b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "303aa066e0c84e099efcb125ce2be2db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Retokenize the data with a max length of 1024\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    model_max_length=1024,\n",
        "    padding_side=\"left\",)\n",
        "#    add_eos_token=True)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "\n",
        "def tokenize(examples):\n",
        "    text = examples[\"0\"]\n",
        "    result = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=1024,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "tokenized_data = data.map(tokenize, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEhuDcyLLDM0"
      },
      "outputs": [],
      "source": [
        "tokenized_train_dataset = tokenized_data['train']\n",
        "tokenized_val_dataset = tokenized_data['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S392doCaLDM1",
        "outputId": "90160462-c025-4f4a-aad1-0e58a83245c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 28792, 16289, 28793, 2012, 16816, 1250, 264, 11308, 3829, 15146, 11429, 477, 521, 1356, 2330, 2245, 28723, 13, 28093, 288, 272, 12493, 3857, 28725, 9131, 544, 272, 2629, 2815, 368, 541, 1300, 297, 272, 2245, 28723, 13, 13, 7435, 28747, 13, 28732, 14537, 8069, 1028, 9572, 28792, 896, 6314, 28735, 2431, 28735, 11379, 28732, 2028, 2540, 28731, 13, 28732, 14537, 8069, 1028, 9572, 28792, 1851, 28730, 1574, 26172, 28730, 10907, 11379, 28732, 4917, 28724, 28731, 13, 28732, 14537, 8069, 1028, 9572, 28792, 28749, 8785, 1906, 28730, 3957, 11379, 28732, 6982, 28731, 13, 28732, 18069, 1018, 9572, 28792, 3701, 832, 4989, 12466, 28730, 9732, 11379, 28732, 14537, 8069, 1028, 28731, 13, 28732, 14537, 8069, 1028, 9572, 28792, 28790, 2185, 1906, 28730, 832, 11379, 28732, 18069, 1018, 28731, 13, 28732, 18069, 1018, 9572, 28792, 14329, 725, 12466, 28730, 3957, 11379, 28732, 20629, 6267, 28731, 13, 28732, 14537, 8069, 1028, 9572, 28792, 8548, 28790, 2255, 28730, 832, 11379, 28732, 20629, 6267, 28731, 13, 28732, 18069, 1018, 9572, 28792, 1336, 1086, 28735, 28730, 21646, 11379, 28732, 22210, 28731, 13, 13, 2083, 28747, 13, 28739, 28760, 2767, 737, 272, 15384, 28782, 28783, 28770, 28770, 28733, 28740, 28740, 28781, 28725, 15384, 28781, 28734, 28770, 28784, 28733, 28740, 28740, 28781, 28725, 15384, 28784, 28770, 28740, 28740, 28733, 28740, 28740, 28781, 28725, 268, 28740, 28774, 28740, 28734, 28733, 28740, 28740, 28781, 28725, 15384, 28750, 28783, 28784, 28781, 28733, 28740, 28740, 28781, 28725, 15384, 28750, 28774, 28740, 28734, 28733, 28740, 28740, 28781, 28725, 304, 268, 28750, 28784, 28782, 28774, 28733, 28740, 28740, 28781, 544, 3215, 395, 12507, 25251, 15251, 611, 28792, 28748, 16289, 28793, 13, 13, 24958, 286, 9982, 2815, 28747, 13, 28732, 2176, 28782, 28783, 28770, 28770, 28733, 28740, 28740, 28781, 28731, 387, 733, 1336, 1086, 28735, 28730, 21646, 28793, 3193, 325, 15827, 282, 25251, 15251, 28731, 13, 28732, 2176, 28781, 28734, 28770, 28784, 28733, 28740, 28740, 28781, 28731, 387, 733, 1336, 1086, 28735, 28730, 21646, 28793, 3193, 325, 15827, 282, 25251, 15251, 28731, 13, 28732, 2176, 28784, 28770, 28740, 28740, 28733, 28740, 28740, 28781, 28731, 387, 733, 1336, 1086, 28735, 28730, 21646, 28793, 3193, 325, 15827, 282, 25251, 15251, 28731, 13, 28732, 28713, 28740, 28774, 28740, 28734, 28733, 28740, 28740, 28781, 28731, 387, 733, 1336, 1086, 28735, 28730, 21646, 28793, 3193, 325, 15827, 282, 25251, 15251, 28731, 13, 28732, 2176, 28750, 28783, 28784, 28781, 28733, 28740, 28740, 28781, 28731, 387, 733, 1336, 1086, 28735, 28730, 21646, 28793, 3193, 325, 15827, 282, 25251, 15251, 28731, 13, 28732, 2176, 28750, 28774, 28740, 28734, 28733, 28740, 28740, 28781, 28731, 387, 733, 1336, 1086, 28735, 28730, 21646, 28793, 3193, 325, 15827, 282, 25251, 15251, 28731, 13, 28732, 28713, 28750, 28784, 28782, 28774, 28733, 28740, 28740, 28781, 28731, 387, 733, 1336, 1086, 28735, 28730, 21646, 28793, 3193, 325, 15827, 282, 25251, 15251, 28731, 2]\n"
          ]
        }
      ],
      "source": [
        "# Print an example of the tokenized_data\n",
        "# The initial \"0\"s are the unk_token used for padding to the left.\n",
        "# The ending \"2\" corresponds to the eos_token\n",
        "print(tokenized_train_dataset[0]['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra9qr-XWLDM1",
        "outputId": "4798df44-58f7-4102-b5c8-f1278268576e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1024\n"
          ]
        }
      ],
      "source": [
        "# The max length has been correctly set to 1024\n",
        "print(len(tokenized_train_dataset[0]['input_ids']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4smWNn4RLDM1"
      },
      "source": [
        "## Trying the base model\n",
        "Let's try for the first time how the base Mistral 7B instruct performs with a casual observation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MsV10dTLDM2"
      },
      "outputs": [],
      "source": [
        "model_input = tokenized_train_dataset[0]['input_ids']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_ya_62ELDM2"
      },
      "outputs": [],
      "source": [
        "eval_prompt = \"\"\"[INST] Imagine being a Knowledge Graph constructor from unstructured text.\n",
        "Following the schema provided, extract all the triples you can find in the text.\n",
        "\n",
        "Schema:\n",
        "(Legislator)-[REPRESENTS]->(StateCode)\n",
        "(Legislator)-[IS_MEMBER_OF]->(Party)\n",
        "(Legislator)-[ELECTED_TO]->(Body)\n",
        "(BillId)-[SPONSORED_BY]->(Legislator)\n",
        "(Legislator)-[VOTED_ON]->(BillId)\n",
        "(BillId)-[REFERRED_TO]->(Committee)\n",
        "(Legislator)-[SERVES_ON]->(Committee)\n",
        "(BillId)-[DEALS_WITH]->(Subject)\n",
        "\n",
        "Context:\n",
        "\"Hey guys, just wanted to share something I found out about the Tennessean and American Samoan representatives.\n",
        "So, both Bob Corker and Chuck Fleischmann are from Tennessee. They're also both Republicans. That's cool! But here's the interesting part: Chuck Fleischmann is in the House, while Bob Corker isn't.\n",
        "And guess what? The American Samoan representative, Amata Coleman Radewagen, is also a Republican. Plus, she's in the House too. Small world, huh?\"[/INST]\n",
        "\n",
        "Extracted Triples:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O600VvnqLDM2",
        "outputId": "3b16df61-ae69-4566-88a6-5670a8e54473"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> [INST] Imagine being a Knowledge Graph constructor from unstructured text.\n",
            "Following the schema provided, extract all the triples you can find in the text.\n",
            "\n",
            "Schema:\n",
            "(Legislator)-[REPRESENTS]->(StateCode)\n",
            "(Legislator)-[IS_MEMBER_OF]->(Party)\n",
            "(Legislator)-[ELECTED_TO]->(Body)\n",
            "(BillId)-[SPONSORED_BY]->(Legislator)\n",
            "(Legislator)-[VOTED_ON]->(BillId)\n",
            "(BillId)-[REFERRED_TO]->(Committee)\n",
            "(Legislator)-[SERVES_ON]->(Committee)\n",
            "(BillId)-[DEALS_WITH]->(Subject)\n",
            "\n",
            "Context:\n",
            "\"Hey guys, just wanted to share something I found out about the Tennessean and American Samoan representatives.\n",
            "So, both Bob Corker and Chuck Fleischmann are from Tennessee. They're also both Republicans. That's cool! But here's the interesting part: Chuck Fleischmann is in the House, while Bob Corker isn't.\n",
            "And guess what? The American Samoan representative, Amata Coleman Radewagen, is also a Republican. Plus, she's in the House too. Small world, huh?\"[/INST]\n",
            "\n",
            "Extracted Triples:\n",
            "(BobCorker)-[REPRESENTS]->(Tennessee)\n",
            "(ChuckFleischmann)-[REPRESENTS]->(Tennessee)\n",
            "(AmataColemanRadewagen)-[REPRESENTS]->(AmericanSamoa)\n",
            "(BobCorker)-[IS_MEMBER_OF]->(Republicans)\n",
            "(ChuckFleischmann)-[IS_MEMBER_OF]->(Republicans)\n",
            "(AmataColemanRadewagen)-[IS_MEMBER_OF]->(Republicans)\n",
            "(BobCorker)-[ELECTED_TO]->(HouseOfRepresentatives)\n",
            "(ChuckFleischmann)-[ELECTED_TO]->(HouseOfRepresentatives)\n",
            "(AmataColemanRadewagen)-[ELECTED_TO]->(HouseOfRepresentatives)</s>\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    model_max_length=1024,\n",
        "    padding_side=\"left\")\n",
        "\n",
        "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_KsqTi_LDM2"
      },
      "source": [
        "# Set up QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHWaKQCeLDM3"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-WvMLIwLDM3",
        "outputId": "42f80408-5d75-4465-f4b7-0ab9612ddbe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MistralForCausalLM(\n",
            "  (model): MistralModel(\n",
            "    (embed_tokens): Embedding(32000, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x MistralDecoderLayer(\n",
            "        (self_attn): MistralSdpaAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): MistralRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): MistralMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Print the model\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1axovts6LDM3"
      },
      "source": [
        "We will apply QLoRA to all the linear layers: `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`, and `lm_head`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L-SDr1pLDM3"
      },
      "outputs": [],
      "source": [
        "# We will apply QLoRA to all the linear layers:\n",
        "linear_layers = [\n",
        "    \"q_proj\",\n",
        "    \"k_proj\",\n",
        "    \"v_proj\",\n",
        "    \"o_proj\",\n",
        "    \"gate_proj\",\n",
        "    \"up_proj\",\n",
        "    \"down_proj\",\n",
        "    \"lm_head\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hHvtj2RLDM3"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules = linear_layers,\n",
        "    #bias=\"none\",\n",
        "    lora_dropout=0.05,  # In QLoRa paper\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NaxF3-HLDM4",
        "outputId": "c8dcebd5-7933-4357-a19d-989f04fa8aa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 21260288 || all params: 3773331456 || trainable%: 0.5634354746703705\n"
          ]
        }
      ],
      "source": [
        "# How many trainable parameters do we have?\n",
        "trainable_params = 0\n",
        "all_param = 0\n",
        "for _, param in model.named_parameters():\n",
        "    all_param += param.numel()\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()\n",
        "print(\n",
        "    f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5L74zKBLDM4"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZax46hrLDM4",
        "outputId": "e4346f1d-f818-47c7-b830-e7f3e3f7b1de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 3:51:12, Epoch 3/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.986600</td>\n",
              "      <td>0.701309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.658600</td>\n",
              "      <td>0.600361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.569500</td>\n",
              "      <td>0.561125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.576000</td>\n",
              "      <td>0.543309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.546400</td>\n",
              "      <td>0.532739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.527600</td>\n",
              "      <td>0.524735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.498200</td>\n",
              "      <td>0.519283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.502900</td>\n",
              "      <td>0.513471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.483600</td>\n",
              "      <td>0.508509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.489700</td>\n",
              "      <td>0.504467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.480900</td>\n",
              "      <td>0.501612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.482300</td>\n",
              "      <td>0.499139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.440100</td>\n",
              "      <td>0.498977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.456600</td>\n",
              "      <td>0.497480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.447200</td>\n",
              "      <td>0.496090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.442500</td>\n",
              "      <td>0.494619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.441400</td>\n",
              "      <td>0.492816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.431000</td>\n",
              "      <td>0.492019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.414200</td>\n",
              "      <td>0.494362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.419700</td>\n",
              "      <td>0.494263</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:204: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:204: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:204: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:204: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:204: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:204: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:204: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:204: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:204: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:204: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:204: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:204: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:204: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:204: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:204: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:204: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:204: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:204: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:204: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:204: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1000, training_loss=0.5147548580169677, metrics={'train_runtime': 13883.8064, 'train_samples_per_second': 0.576, 'train_steps_per_second': 0.072, 'total_flos': 3.50548150714368e+17, 'train_loss': 0.5147548580169677, 'epoch': 3.3333333333333335})"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import transformers\n",
        "from datetime import datetime\n",
        "\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "\n",
        "t_args = transformers.TrainingArguments(\n",
        "    output_dir=\"Prova1\",\n",
        "    warmup_steps=5,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_accumulation_steps=4,\n",
        "    max_steps=1000,\n",
        "    learning_rate=2.5e-5, # Want about 10x smaller than the Mistral learning rate\n",
        "    logging_steps=50,\n",
        "    bf16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    logging_dir=\"./logs\",        # Directory for storing logs\n",
        "    save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
        "    save_steps=50,                # Save checkpoints every 50 steps\n",
        "    eval_strategy=\"steps\", # Evaluate the model every logging step\n",
        "    eval_steps=50               # Evaluate and save checkpoints every 50 steps\n",
        "    #do_eval=True,                # Perform evaluation at the end of training\n",
        ")\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    args= t_args,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# Silence the warnings\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning for Few Shot prompting\n",
        "We are now going to repeat the exact same operations, but using the dataset containing in its observations examples."
      ],
      "metadata": {
        "id": "pgza7baWPhkG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySzz3zaZPeNV"
      },
      "source": [
        "## Load Dataset and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqrhL1vEPeNW",
        "outputId": "37b6fcaa-a966-4b40-eff3-4a341e9a81f5",
        "colab": {
          "referenced_widgets": [
            "a095d514c79b432d9543a6a5a89ff6ac",
            "d7e6de51bbb64bfea5c7e90dcddb644b",
            "b08bda1e5b5d4c7ca77a242fc111897a",
            "5721e5c177e948f7ad968b95e71a2eeb"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a095d514c79b432d9543a6a5a89ff6ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/5.85M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7e6de51bbb64bfea5c7e90dcddb644b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/955k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b08bda1e5b5d4c7ca77a242fc111897a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5721e5c177e948f7ad968b95e71a2eeb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['0'],\n",
            "        num_rows: 2400\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['0'],\n",
            "        num_rows: 400\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Load Dataset\n",
        "data = load_dataset(\"Giardooo/KG_constructor_FS\")\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBjwYIB5PeNW",
        "outputId": "a868c7bc-9c7b-43a7-f573-6c65a64b7831",
        "colab": {
          "referenced_widgets": [
            "1ea3b9fde2604cb88d7df947bc4b356f"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ea3b9fde2604cb88d7df947bc4b356f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Login to Hugginface\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdchGAU3PeNX",
        "outputId": "333f732a-33ca-4914-a262-9bc848aec355",
        "colab": {
          "referenced_widgets": [
            "1613f4e6a69e4cf691cff71ac54cfb81",
            "d3369eafe2794697a13754a351bc1b44",
            "3050152832234c2aa7aa894b7e62aeff",
            "1d89b75889144227901b278c9fa93f5c",
            "3e442183ea6c4e2985795183b69e02bd",
            "235616c881c243ea8bac782da67e238d",
            "f84e0a52fbfa4bf8b0b2d96e949687c0"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1613f4e6a69e4cf691cff71ac54cfb81",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3369eafe2794697a13754a351bc1b44",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3050152832234c2aa7aa894b7e62aeff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d89b75889144227901b278c9fa93f5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e442183ea6c4e2985795183b69e02bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "235616c881c243ea8bac782da67e238d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f84e0a52fbfa4bf8b0b2d96e949687c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load Model\n",
        "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    # Load the 4 bit quantized model\n",
        "    load_in_4bit=True,\n",
        "    # Use nested quantization\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    # Define the quantization data type to NF4\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    # Set the computational type to bf16 for speed-up\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBq7MwXOPeNX"
      },
      "source": [
        "## Set up the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTDb6SXxPeNY",
        "outputId": "139c3367-d5bc-4cec-f721-1b3f2243932f",
        "colab": {
          "referenced_widgets": [
            "72cfe3022a3e4ace8eb9b5b75c502cb7",
            "98e4d03c51a64b86b6d1fc6e4502c303",
            "1a51b7eb80f84cdeb3c6002bff8b91e7",
            "cbcdc573b6f246c3b66e8d74c30a875a"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72cfe3022a3e4ace8eb9b5b75c502cb7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98e4d03c51a64b86b6d1fc6e4502c303",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a51b7eb80f84cdeb3c6002bff8b91e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbcdc573b6f246c3b66e8d74c30a875a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Set up tokenizer without max tokens\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    # Set up Mistral tokenizer\n",
        "    base_model_id,\n",
        "    # Since Mistral is decoder-only, left pad is preferred\n",
        "    padding_side=\"left\",\n",
        "    # End Of Sequence token\n",
        "    add_eos_token=True)\n",
        "\n",
        "# Set pad token to unk_token\n",
        "# The default pad_token = eos_token results in the LLM forgetting the appropriate usage of the eos token, making it never stop generating text\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "\n",
        "def tokenize(examples):\n",
        "    # Extract the column of the dataset containing the text\n",
        "    text = examples[\"0\"]\n",
        "    result = tokenizer(\n",
        "        text,\n",
        "        # truncation=True,\n",
        "    )\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fjiopv-XPeNY",
        "outputId": "55a52dbf-b5a5-48d3-9e69-50f382ac542a",
        "colab": {
          "referenced_widgets": [
            "51f42a6379704fc082c0b26d6e742245",
            "14a49788121641c7b90d6e2516be21c8"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51f42a6379704fc082c0b26d6e742245",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14a49788121641c7b90d6e2516be21c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Tokenize the dataset\n",
        "tokenized_data = data.map(tokenize, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN_cLpz1PeNZ"
      },
      "source": [
        "# Choose max_length\n",
        "Let's check the length (in token) of our observations. In this way we are going to choose the max_length parameter of our tokenizer.<br>\n",
        "Of course, we expect that the length of our observatins is greater than before, since we added 3 examples in each of them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFKlK742PeNZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPbx7-4QPeNZ",
        "outputId": "48725319-6803-42cc-e601-685fbf027174"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2800\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def plot_data_lengths(train, val):\n",
        "    lengths = [len(x['input_ids']) for x in train]\n",
        "    lengths += [len(x['input_ids']) for x in val]\n",
        "    print(len(lengths))\n",
        "\n",
        "    # Plotting the histogram\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
        "    plt.xlabel('Num of tokens')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of Num of tokens')\n",
        "    plt.show()\n",
        "\n",
        "plot_data_lengths(tokenized_data['train'], tokenized_data['test'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYNFj0qwPeNa"
      },
      "source": [
        "As we expected, the observations got lengthier. Now, 3072 seems a reasonable max_length for our tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd8ZrYjiPeNa",
        "outputId": "430159de-b647-4d49-a267-0ce0de65225a",
        "colab": {
          "referenced_widgets": [
            "309ac192da6f46a6bcf330864dd6446d",
            "baab866d789c4c3ca690404f5e43fb89"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "309ac192da6f46a6bcf330864dd6446d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "baab866d789c4c3ca690404f5e43fb89",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Retokenize the data with a max length of 3072\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    model_max_length=3072,\n",
        "    padding_side=\"left\",)\n",
        "#    add_eos_token=True)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "\n",
        "def tokenize(examples):\n",
        "    text = examples[\"0\"]\n",
        "    result = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=3072,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "tokenized_data = data.map(tokenize, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52iTztC9PeNa"
      },
      "outputs": [],
      "source": [
        "tokenized_train_dataset = tokenized_data['train']\n",
        "tokenized_val_dataset = tokenized_data['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "E1AInJvnPeNb",
        "outputId": "2ef5194a-dd78-454e-aa2f-97c628b4aef5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 28792, 16289, 28793, 2012, 16816, 1250, 264, 11308, 3829, 15146, 11429, 477, 521, 1356, 2330, 2245, 28723, 13, 28093, 288, 272, 12493, 3857, 28725, 9131, 544, 272, 2629, 2815, 368, 541, 1300, 297, 272, 2245, 28723, 13, 13, 7435, 28747, 13, 28732, 14537, 8069, 1028, 9572, 28792, 896, 6314, 28735, 2431, 28735, 11379, 28732, 2028, 2540, 28731, 13, 28732, 14537, 8069, 1028, 9572, 28792, 1851, 28730, 1574, 26172, 28730, 10907, 11379, 28732, 4917, 28724, 28731, 13, 28732, 14537, 8069, 1028, 9572, 28792, 28749, 8785, 1906, 28730, 3957, 11379, 28732, 6982, 28731, 13, 28732, 18069, 1018, 9572, 28792, 3701, 832, 4989, 12466, 28730, 9732, 11379, 28732, 14537, 8069, 1028, 28731, 13, 28732, 14537, 8069, 1028, 9572, 28792, 28790, 2185, 1906, 28730, 832, 11379, 28732, 18069, 1018, 28731, 13, 28732, 18069, 1018, 9572, 28792, 14329, 725, 12466, 28730, 3957, 11379, 28732, 20629, 6267, 28731, 13, 28732, 14537, 8069, 1028, 9572, 28792, 8548, 28790, 2255, 28730, 832, 11379, 28732, 20629, 6267, 28731, 13, 28732, 18069, 1018, 9572, 28792, 1336, 1086, 28735, 28730, 21646, 11379, 28732, 22210, 28731, 13, 13, 15423, 460, 741, 9254, 28747, 13, 2083, 28747, 13, 28739, 1163, 288, 582, 3154, 28725, 478, 506, 272, 3296, 28733, 1769, 28733, 1237, 28733, 1402, 4009, 356, 4875, 15384, 28750, 28783, 28734, 28781, 28733, 28740, 28740, 28781, 28725, 690, 14685, 395, 1665, 304, 1862, 3058, 6933, 28723, 851, 4875, 659, 264, 1043, 1274, 302, 12439, 734, 28725, 2490, 9360, 2332, 326, 2047, 28725, 1054, 3679, 393, 1009, 27526, 28725, 7149, 13870, 27528, 28725, 3189, 16015, 263, 28725, 15444, 13091, 308, 6019, 28725, 9142, 13343, 555, 28724, 28725, 475, 1327, 382, 1292, 1294, 28725, 304, 11074, 28863, 28714, 420, 8017, 282, 3026, 28723, 20066, 15013, 286, 354, 680, 4162, 390, 590, 26061, 2781, 13, 13, 24958, 286, 9982, 2815, 28747, 13, 28732, 28713, 28740, 28781, 28734, 28734, 28733, 28740, 28740, 28781, 28731, 387, 733, 1336, 1086, 28735, 28730, 21646, 28793, 3193, 325, 2028, 304, 1862, 3058, 6933, 28731, 13, 28732, 2176, 28750, 28783, 28734, 28781, 28733, 28740, 28740, 28781, 28731, 387, 733, 3701, 832, 4989, 12466, 28730, 9732, 28793, 3193, 325, 28755, 2474, 2332, 326, 2047, 1143, 13, 28732, 2176, 28750, 28783, 28734, 28781, 28733, 28740, 28740, 28781, 28731, 387, 733, 3701, 832, 4989, 12466, 28730, 9732, 28793, 3193, 325, 28828, 3679, 393, 1009, 27526, 28731, 13, 28732, 2176, 28750, 28783, 28734, 28781, 28733, 28740, 28740, 28781, 28731, 387, 733, 3701, 832, 4989, 12466, 28730, 9732, 28793, 3193, 325, 28755, 1061, 13870, 27528, 28731, 13, 28732, 2176, 28750, 28783, 28734, 28781, 28733, 28740, 28740, 28781, 28731, 387, 733, 3701, 832, 4989, 12466, 28730, 9732, 28793, 3193, 325, 6017, 16015, 263, 28731, 13, 28732, 2176, 28750, 28783, 28734, 28781, 28733, 28740, 28740, 28781, 28731, 387, 733, 3701, 832, 4989, 12466, 28730, 9732, 28793, 3193, 325, 2707, 276, 13091, 308, 6019, 28731, 13, 28732, 2176, 28750, 28783, 28734, 28781, 28733, 28740, 28740, 28781, 28731, 387, 733, 3701, 832, 4989, 12466, 28730, 9732, 28793, 3193, 325, 28755, 6527, 13343, 555, 28724, 28731, 13, 28732, 2176, 28750, 28783, 28734, 28781, 28733, 28740, 28740, 28781, 28731, 387, 733, 3701, 832, 4989, 12466, 28730, 9732, 28793, 3193, 325, 28798, 1327, 382, 1292, 1294, 28731, 13, 28732, 2176, 28750, 28783, 28734, 28781, 28733, 28740, 28740, 28781, 28731, 387, 733, 3701, 832, 4989, 12466, 28730, 9732, 28793, 3193, 325, 28754, 28708, 28863, 28714, 420, 8017, 282, 3026, 28731, 13, 28732, 2176, 28750, 28783, 28734, 28781, 28733, 28740, 28740, 28781, 28731, 387, 733, 3701, 832, 4989, 12466, 28730, 9732, 28793, 3193, 325, 28828, 3679, 393, 1009, 27526, 28731, 13, 28732, 2176, 28750, 28783, 28734, 28781, 28733, 28740, 28740, 28781, 28731, 387, 733, 3701, 832, 4989, 12466, 28730, 9732, 28793, 3193, 325, 28755, 2474, 2332, 326, 2047, 1143, 13, 28732, 2176, 28750, 28783, 28734, 28781, 28733, 28740, 28740, 28781, 28731, 387, 733, 3701, 832, 4989, 12466, 28730, 9732, 28793, 3193, 325, 28755, 1061, 13870, 27528, 28731, 13, 28732, 2176, 28750, 28783, 28734, 28781, 28733, 28740, 28740, 28781, 28731, 387, 733, 3701, 832, 4989, 12466, 28730, 9732, 28793, 3193, 325, 28755, 6527, 13343, 555, 28724, 28731, 13, 28732, 2176, 28750, 28783, 28734, 28781, 28733, 28740, 28740, 28781, 28731, 387, 733, 3701, 832, 4989, 12466, 28730, 9732, 28793, 3193, 325, 28754, 28708, 28863, 28714, 420, 8017, 282, 3026, 28731, 13, 28732, 2176, 28750, 28783, 28734, 28781, 28733, 28740, 28740, 28781, 28731, 387, 733, 3701, 832, 4989, 12466, 28730, 9732, 28793, 3193, 325, 28798, 1327, 382, 1292, 1294, 28731, 13, 28732, 2176, 28750, 28783, 28734, 28781, 28733, 28740, 28740, 28781, 28731, 387, 733, 3701, 832, 4989, 12466, 28730, 9732, 28793, 3193, 325, 2707, 276, 13091, 308, 6019, 28731, 13, 28732, 2176, 28750, 28783, 28734, 28781, 28733, 28740, 28740, 28781, 28731, 387, 733, 3701, 832, 4989, 12466, 28730, 9732, 28793, 3193, 325, 6017, 16015, 263, 28731, 13, 28732, 2176, 28750, 28783, 28734, 28781, 28733, 28740, 28740, 28781, 28731, 387, 733, 1336, 1086, 28735, 28730, 21646, 28793, 3193, 325, 2028, 304, 1862, 3058, 6933, 28731, 13, 22843, 13, 2083, 28747, 13, 28739, 4781, 2044, 1197, 2740, 563, 15128, 1294, 399, 770, 28727, 5786, 5915, 1269, 1335, 8063, 390, 272, 11150, 12439, 271, 302, 1712, 16843, 297, 272, 4594, 28747, 15384, 28750, 28784, 28770, 28782, 28733, 28740, 28740, 28781, 28725, 15384, 28781, 28781, 28782, 28782, 28733, 28740, 28740, 28781, 28725, 304, 15384, 28782, 28750, 28750, 28774, 28733, 28740, 28740, 28781, 28723, 415, 12294, 15405, 1028, 295, 2014, 477, 272, 2556, 4157, 17542, 325, 2109, 28731, 304, 659, 750, 11401, 298, 272, 4594, 302, 17891, 5087, 28723, 14862, 1250, 264, 4292, 302, 272, 12294, 6636, 28725, 399, 770, 28727, 5786, 403, 835, 5915, 7298, 297, 272, 4594, 486, 8828, 2984, 5507, 753, 8885, 2984, 2035, 304, 23285, 318, 2892, 25062, 28723, 4023, 318, 2892, 25062, 10651, 6247, 28725, 630, 659, 750, 11401, 298, 272, 4594, 304, 349, 459, 11795, 601, 395, 707, 4150, 611, 13, 13, 24958, 286, 9982, 2815, 28747, 13, 28732, 28758, 16224, 318, 2892, 25062, 28731, 387, 733, 896, 6314, 28735, 2431, 28735, 28793, 3193, 325, 5194, 28731, 13, 28732, 28758, 16224, 318, 2892, 25062, 28731, 387, 733, 28749, 8785, 1906, 28730, 3957, 28793, 3193, 325, 28769, 1284, 28731, 13, 28732, 28796, 269, 2984, 2035, 28731, 387, 733, 896, 6314, 28735, 2431, 28735, 28793, 3193, 325, 5194, 28731, 13, 28732, 28796, 269, 2984, 2035, 28731, 387, 733, 1851, 28730, 1574, 26172, 28730, 10907, 28793, 3193, 325, 4781, 651, 276, 28731, 13, 28732, 28796, 269, 2984, 2035, 28731, 387, 733, 28749, 8785, 1906, 28730, 3957, 28793, 3193, 325, 28769, 1284, 28731, 13, 28732, 7118, 563, 15128, 1294, 399, 770, 28727, 5786, 28731, 387, 733, 896, 6314, 28735, 2431, 28735, 28793, 3193, 325, 2109, 28731, 13, 28732, 7118, 563, 15128, 1294, 399, 770, 28727, 5786, 28731, 387, 733, 1851, 28730, 1574, 26172, 28730, 10907, 28793, 3193, 325, 4781, 651, 276, 28731, 13, 28732, 7118, 563, 15128, 1294, 399, 770, 28727, 5786, 28731, 387, 733, 28749, 8785, 1906, 28730, 3957, 28793, 3193, 325, 28769, 1284, 28731, 13, 28732, 2176, 28750, 28784, 28770, 28782, 28733, 28740, 28740, 28781, 28731, 387, 733, 3701, 832, 4989, 12466, 28730, 9732, 28793, 3193, 325, 7118, 563, 15128, 1294, 399, 770, 28727, 5786, 28731, 13, 28732, 2176, 28750, 28784, 28770, 28782, 28733, 28740, 28740, 28781, 28731, 387, 733, 3701, 832, 4989, 12466, 28730, 9732, 28793, 3193, 325, 7118, 563, 15128, 1294, 399, 770, 28727, 5786, 28731, 13, 28732, 2176, 28781, 28781, 28782, 28782, 28733, 28740, 28740, 28781, 28731, 387, 733, 3701, 832, 4989, 12466, 28730, 9732, 28793, 3193, 325, 7118, 563, 15128, 1294, 399, 770, 28727, 5786, 28731, 13, 28732, 2176, 28781, 28781, 28782, 28782, 28733, 28740, 28740, 28781, 28731, 387, 733, 3701, 832, 4989, 12466, 28730, 9732, 28793, 3193, 325, 7118, 563, 15128, 1294, 399, 770, 28727, 5786, 28731, 13, 28732, 2176, 28782, 28750, 28750, 28774, 28733, 28740, 28740, 28781, 28731, 387, 733, 3701, 832, 4989, 12466, 28730, 9732, 28793, 3193, 325, 7118, 563, 15128, 1294, 399, 770, 28727, 5786, 28731, 13, 28732, 2176, 28782, 28750, 28750, 28774, 28733, 28740, 28740, 28781, 28731, 387, 733, 3701, 832, 4989, 12466, 28730, 9732, 28793, 3193, 325, 7118, 563, 15128, 1294, 399, 770, 28727, 5786, 28731, 13, 22843, 13, 2083, 28747, 13, 28739, 348, 15774, 745, 7982, 356, 272, 399, 864, 348, 13, 7516, 8924, 734, 28725, 16843, 268, 28740, 28770, 28774, 28733, 28740, 28740, 28781, 28725, 15384, 28770, 28787, 28750, 28774, 28733, 28740, 28740, 28781, 28725, 15384, 28750, 28774, 28750, 28740, 28733, 28740, 28740, 28781, 28725, 268, 28750, 28770, 28783, 28750, 28733, 28740, 28740, 28781, 28725, 304, 15384, 28740, 28774, 28750, 28782, 28733, 28740, 28740, 28781, 460, 544, 9045, 356, 624, 2278, 9067, 28747, 5714, 3332, 28723, 2326, 579, 1188, 4501, 1250, 5907, 298, 456, 13040, 1834, 28725, 378, 28742, 28713, 3081, 369, 813, 3437, 2528, 349, 2526, 1170, 8918, 821, 2270, 611, 13, 13, 24958, 286, 9982, 2815, 28747, 13, 28732, 28713, 28740, 28770, 28774, 28733, 28740, 28740, 28781, 28731, 387, 733, 1336, 1086, 28735, 28730, 21646, 28793, 3193, 325, 15774, 745, 3332, 28731, 13, 28732, 2176, 28770, 28787, 28750, 28774, 28733, 28740, 28740, 28781, 28731, 387, 733, 1336, 1086, 28735, 28730, 21646, 28793, 3193, 325, 15774, 745, 3332, 28731, 13, 28732, 2176, 28750, 28774, 28750, 28740, 28733, 28740, 28740, 28781, 28731, 387, 733, 1336, 1086, 28735, 28730, 21646, 28793, 3193, 325, 15774, 745, 3332, 28731, 13, 28732, 28713, 28750, 28770, 28783, 28750, 28733, 28740, 28740, 28781, 28731, 387, 733, 1336, 1086, 28735, 28730, 21646, 28793, 3193, 325, 15774, 745, 3332, 28731, 13, 28732, 2176, 28740, 28774, 28750, 28782, 28733, 28740, 28740, 28781, 28731, 387, 733, 1336, 1086, 28735, 28730, 21646, 28793, 3193, 325, 15774, 745, 3332, 28731, 13, 22843, 13, 2083, 13, 28739, 28760, 2767, 737, 272, 15384, 28782, 28783, 28770, 28770, 28733, 28740, 28740, 28781, 28725, 15384, 28781, 28734, 28770, 28784, 28733, 28740, 28740, 28781, 28725, 15384, 28784, 28770, 28740, 28740, 28733, 28740, 28740, 28781, 28725, 268, 28740, 28774, 28740, 28734, 28733, 28740, 28740, 28781, 28725, 15384, 28750, 28783, 28784, 28781, 28733, 28740, 28740, 28781, 28725, 15384, 28750, 28774, 28740, 28734, 28733, 28740, 28740, 28781, 28725, 304, 268, 28750, 28784, 28782, 28774, 28733, 28740, 28740, 28781, 544, 3215, 395, 12507, 25251, 15251, 611, 28792, 28748, 16289, 28793, 13, 13, 24958, 286, 9982, 2815, 28747, 13, 28732, 2176, 28782, 28783, 28770, 28770, 28733, 28740, 28740, 28781, 28731, 387, 733, 1336, 1086, 28735, 28730, 21646, 28793, 3193, 325, 15827, 282, 25251, 15251, 28731, 13, 28732, 2176, 28781, 28734, 28770, 28784, 28733, 28740, 28740, 28781, 28731, 387, 733, 1336, 1086, 28735, 28730, 21646, 28793, 3193, 325, 15827, 282, 25251, 15251, 28731, 13, 28732, 2176, 28784, 28770, 28740, 28740, 28733, 28740, 28740, 28781, 28731, 387, 733, 1336, 1086, 28735, 28730, 21646, 28793, 3193, 325, 15827, 282, 25251, 15251, 28731, 13, 28732, 28713, 28740, 28774, 28740, 28734, 28733, 28740, 28740, 28781, 28731, 387, 733, 1336, 1086, 28735, 28730, 21646, 28793, 3193, 325, 15827, 282, 25251, 15251, 28731, 13, 28732, 2176, 28750, 28783, 28784, 28781, 28733, 28740, 28740, 28781, 28731, 387, 733, 1336, 1086, 28735, 28730, 21646, 28793, 3193, 325, 15827, 282, 25251, 15251, 28731, 13, 28732, 2176, 28750, 28774, 28740, 28734, 28733, 28740, 28740, 28781, 28731, 387, 733, 1336, 1086, 28735, 28730, 21646, 28793, 3193, 325, 15827, 282, 25251, 15251, 28731, 13, 28732, 28713, 28750, 28784, 28782, 28774, 28733, 28740, 28740, 28781, 28731, 387, 733, 1336, 1086, 28735, 28730, 21646, 28793, 3193, 325, 15827, 282, 25251, 15251, 28731, 2]\n"
          ]
        }
      ],
      "source": [
        "# Print an example of the tokenized_data\n",
        "# The initial \"0\"s are the unk_token used for padding to the left.\n",
        "# The ending \"2\" corresponds to the eos_token\n",
        "print(tokenized_train_dataset[0]['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuDGJUI2PeNb",
        "outputId": "f50d0f3c-7277-49ab-f2d9-dcbc84f8d1d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3072\n"
          ]
        }
      ],
      "source": [
        "# The max length has been correctly set to 3072\n",
        "print(len(tokenized_train_dataset[0]['input_ids']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tByn-OW3PeNb"
      },
      "source": [
        "# Set up QLoRA\n",
        "These are exactly the same cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cy5vQ_KNPeNc"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWRpNKmRPeNc",
        "outputId": "96c0e1e4-3505-48be-a8c4-6ef400581ec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MistralForCausalLM(\n",
            "  (model): MistralModel(\n",
            "    (embed_tokens): Embedding(32000, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x MistralDecoderLayer(\n",
            "        (self_attn): MistralAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): MistralRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): MistralMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Print the model\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dmEi9o2PeNc"
      },
      "source": [
        "We will apply QLoRA to all the linear layers: `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`, and `lm_head`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ex-1y0AmPeNd"
      },
      "outputs": [],
      "source": [
        "# We will apply QLoRA to all the linear layers:\n",
        "linear_layers = [\n",
        "    \"q_proj\",\n",
        "    \"k_proj\",\n",
        "    \"v_proj\",\n",
        "    \"o_proj\",\n",
        "    \"gate_proj\",\n",
        "    \"up_proj\",\n",
        "    \"down_proj\",\n",
        "    \"lm_head\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGWiqfBSPeNd"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules = linear_layers,\n",
        "    #bias=\"none\",\n",
        "    lora_dropout=0.05,  # In QLoRa paper\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgAvCPuXPeNd",
        "outputId": "214771a1-891d-42ca-a41b-4b8cef64e03e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 21260288 || all params: 3773331456 || trainable%: 0.5634354746703705\n"
          ]
        }
      ],
      "source": [
        "# How many trainable parameters do we have?\n",
        "trainable_params = 0\n",
        "all_param = 0\n",
        "for _, param in model.named_parameters():\n",
        "    all_param += param.numel()\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()\n",
        "print(\n",
        "    f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "213hOHegPeNd"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZkKJqMqPeNd",
        "outputId": "16f51453-0d6b-4c6f-e369-81d2f78ef07f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='401' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 401/1000 5:38:37 < 8:28:20, 0.02 it/s, Epoch 1.33/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.942300</td>\n",
              "      <td>0.778176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.712600</td>\n",
              "      <td>0.675742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.640500</td>\n",
              "      <td>0.647505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.606800</td>\n",
              "      <td>0.622850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.567800</td>\n",
              "      <td>0.605893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.533300</td>\n",
              "      <td>0.595291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.484800</td>\n",
              "      <td>0.599950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.460800</td>\n",
              "      <td>0.600457</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Silence the warnings\u001b[39;00m\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1991\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1989\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1991\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1992\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1995\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1996\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2327\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2326\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2327\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2330\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2331\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2332\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2333\u001b[0m ):\n\u001b[1;32m   2334\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2335\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3452\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3450\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3452\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:2196\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2196\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "from datetime import datetime\n",
        "\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "\n",
        "t_args = transformers.TrainingArguments(\n",
        "    output_dir=\"Prova_FS\",\n",
        "    warmup_steps=5,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_accumulation_steps=4,\n",
        "    max_steps=1000,\n",
        "    learning_rate=2.5e-5, # Want about 10x smaller than the Mistral learning rate\n",
        "    logging_steps=50,\n",
        "    bf16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    logging_dir=\"./logs\",        # Directory for storing logs\n",
        "    save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
        "    save_steps=50,                # Save checkpoints every 50 steps\n",
        "    eval_strategy=\"steps\", # Evaluate the model every logging step\n",
        "    eval_steps=50               # Evaluate and save checkpoints every 50 steps\n",
        "    #do_eval=True,                # Perform evaluation at the end of training\n",
        ")\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    args= t_args,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# Silence the warnings\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error you see are given by the fact that I manually interrupted the training as the model began overfitting.<br>\n",
        "Furthermore, as you can see, the training of a model with these lengthier observations requires way more time (40 minutes for each 50 steps), and I do not have the resources to train it till the end."
      ],
      "metadata": {
        "id": "x21tiKz7QTKR"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}