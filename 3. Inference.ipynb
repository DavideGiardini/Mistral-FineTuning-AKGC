{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCnb_TS_RR2b"
      },
      "source": [
        "# Inference\n",
        "Now that we have our finetuned model, we are going to generate the triples from the test text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzKS_ZaFRR2d"
      },
      "source": [
        "## Import libraries and LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "2dcixmHNRR2d",
        "outputId": "167f7ce3-cfce-46fb-9cf5-272c66b35473"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Downloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Downloading fonttools-4.53.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.2/162.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.24.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Downloading matplotlib-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.53.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
            "Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.53.0 kiwisolver-1.4.5 matplotlib-3.9.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U datasets scipy ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c8d1034b903a4974bca050685feeb18a"
          ]
        },
        "id": "BsVgNO0TRR2f",
        "outputId": "ec07944c-fed9-46a1-db5e-389c74d5ad54"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8d1034b903a4974bca050685feeb18a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "883619edfea640199ab2f70679d5b0d7",
            "1724533fb3db49f7beb8ea7dc6ae29b7",
            "0a502a284dad4189b9418558be9ca9b5",
            "1b35de3d403f49378dcbc910fec4eb1c",
            "ae8baa56687a4c409329ceaafa2a4c54",
            "74e97dc35e5849388003c2038faffc57",
            "257a3c61b89c432bb53685bf05289d9e"
          ]
        },
        "id": "4zJ34S8FRR2f",
        "outputId": "acca618a-c0bd-4f24-c0eb-83a5532caf69"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "883619edfea640199ab2f70679d5b0d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1724533fb3db49f7beb8ea7dc6ae29b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a502a284dad4189b9418558be9ca9b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b35de3d403f49378dcbc910fec4eb1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae8baa56687a4c409329ceaafa2a4c54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74e97dc35e5849388003c2038faffc57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "257a3c61b89c432bb53685bf05289d9e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,  # Mistral, same as before\n",
        "    quantization_config=bnb_config,  # Same quantization config as before\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "b9db0cc724cb459797a988b0772da00e",
            "b9a2ca3e593a43ef9aecc7884c775b21",
            "a874be0e6cc34d0488f0c3168e1e385c",
            "60676781b375423186387d696951526f"
          ]
        },
        "id": "zUX3dykYRR2g",
        "outputId": "d97b9b61-be48-486a-bebb-0f5a13efb56b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9db0cc724cb459797a988b0772da00e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9a2ca3e593a43ef9aecc7884c775b21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a874be0e6cc34d0488f0c3168e1e385c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60676781b375423186387d696951526f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    add_bos_token=False,\n",
        "    add_eos_token=False,\n",
        "    trust_remote_code=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzU9L2ieRR2g"
      },
      "source": [
        "# Load Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9n1TV_URR2h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAi7QMegRR2h"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"test.csv\")[\"0\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMu2GEgYRR2h"
      },
      "outputs": [],
      "source": [
        "test_df = list(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpadYTC6RR2h"
      },
      "outputs": [],
      "source": [
        "formatted_test_df = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKSgXRyRRR2i"
      },
      "outputs": [],
      "source": [
        "# For the base model, we are going to add this sentence in order to give it instructions on how to format the triples\n",
        "sentence = \"\"\"Following the schema provided, extract all the triples you can find in the text.\n",
        "Make sure to mantain the formatting of the schema: \"(subject)-[PREDICATE]->(Object)\" \"\"\"\n",
        "sentence = sentence.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfNOuFQ8RR2i",
        "outputId": "b2399d68-6bdb-4929-86df-92e3f5fdb2cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<s>[INST]Imagine being a Knowledge Graph constructor from unstructured text.\\nFollowing the schema provided, extract all the triples you can find in the text.\\n\\nSchema:\\n(Legislator)-[REPRESENTS]->(StateCode)\\n(Legislator)-[IS_MEMBER_OF]->(Party)\\n(Legislator)-[ELECTED_TO]->(Body)\\n(BillId)-[SPONSORED_BY]->(Legislator)\\n(Legislator)-[VOTED_ON]->(BillId)\\n(BillId)-[REFERRED_TO]->(Committee)\\n(Legislator)-[SERVES_ON]->(Committee)\\n(BillId)-[DEALS_WITH]->(Subject)\\n\\nContext:\\n\"Hi and welcome to our show about very interesting bills.\\nToday\\'s bill is the hr1818-114 bill. This bill was proposed by Ken Calvert, a Republican representative from California, and Grace Napolitano, a representative from California who is elected to the House. These two representatives are from the same state and share the sponsorship of the same bill.\"[/INST]\\n\\nExtracted Triples:\\n(Grace Napolitano) - [REPRESENTS] -> (CA)\\n(Grace Napolitano) - [ELECTED_TO] -> (House)\\n(Ken Calvert) - [REPRESENTS] -> (CA)\\n(Ken Calvert) - [IS_MEMBER_OF] -> (Republican)\\n(Ken Calvert) - [ELECTED_TO] -> (House)\\n(hr1818-114) - [SPONSORED_BY] -> (Ken Calvert)\\n(hr1818-114) - [SPONSORED_BY] -> (Grace Napolitano)\\n(hr1818-114) - [SPONSORED_BY] -> (Ken Calvert)\\n(hr1818-114) - [SPONSORED_BY] -> (Grace Napolitano)</s>'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXlAIwR0RR2i",
        "outputId": "75cdd373-2e32-4a75-e928-3951c8095ca0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [00:00<00:00, 182143.26it/s]\n"
          ]
        }
      ],
      "source": [
        "for el in tqdm(test_df):\n",
        "    new_diz = {}\n",
        "    splitted_el = el.split(\"[/INST]\\n\\nExtracted Triples:\")\n",
        "    text = splitted_el[0][3:].strip()\n",
        "    text = text.split(\"Following the schema provided, extract all the triples you can find in the text.\")\n",
        "    text = sentence.join(text)\n",
        "    text += \"[/INST]\\n\\nExtracted Triples:\"\n",
        "    new_diz[\"text\"] = text\n",
        "    new_diz[\"triples\"] = splitted_el[1][:-4].strip()\n",
        "    formatted_test_df.append(new_diz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtMnIc2CRR2i",
        "outputId": "c28d8b1a-9585-4ceb-85e4-66f84d8f4c8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': '[INST]Imagine being a Knowledge Graph constructor from unstructured text.\\nFollowing the schema provided, extract all the triples you can find in the text.\\nMake sure to mantain the formatting of the schema: \"(subject)-[PREDICATE]->(Object)\"\\n\\nSchema:\\n(Legislator)-[REPRESENTS]->(StateCode)\\n(Legislator)-[IS_MEMBER_OF]->(Party)\\n(Legislator)-[ELECTED_TO]->(Body)\\n(BillId)-[SPONSORED_BY]->(Legislator)\\n(Legislator)-[VOTED_ON]->(BillId)\\n(BillId)-[REFERRED_TO]->(Committee)\\n(Legislator)-[SERVES_ON]->(Committee)\\n(BillId)-[DEALS_WITH]->(Subject)\\n\\nContext:\\n\"Hi and welcome to our show about very interesting bills.\\nToday\\'s bill is the hr1818-114 bill. This bill was proposed by Ken Calvert, a Republican representative from California, and Grace Napolitano, a representative from California who is elected to the House. These two representatives are from the same state and share the sponsorship of the same bill.\"[/INST]\\n\\nExtracted Triples:',\n",
              " 'triples': '(Grace Napolitano) - [REPRESENTS] -> (CA)\\n(Grace Napolitano) - [ELECTED_TO] -> (House)\\n(Ken Calvert) - [REPRESENTS] -> (CA)\\n(Ken Calvert) - [IS_MEMBER_OF] -> (Republican)\\n(Ken Calvert) - [ELECTED_TO] -> (House)\\n(hr1818-114) - [SPONSORED_BY] -> (Ken Calvert)\\n(hr1818-114) - [SPONSORED_BY] -> (Grace Napolitano)\\n(hr1818-114) - [SPONSORED_BY] -> (Ken Calvert)\\n(hr1818-114) - [SPONSORED_BY] -> (Grace Napolitano)'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "formatted_test_df[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZSVeHAzRR2j"
      },
      "source": [
        "# Generate responses (base model)\n",
        "Generate the triples starting from the test set for Mistral 7B Instruct non-finetuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J4TMGkERR2j",
        "outputId": "251ff539-78a8-4adb-a572-2486e10dbba5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MistralForCausalLM(\n",
              "  (model): MistralModel(\n",
              "    (embed_tokens): Embedding(32000, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x MistralDecoderLayer(\n",
              "        (self_attn): MistralSdpaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): MistralRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): MistralMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    model_max_length=1024,\n",
        "    padding_side=\"left\")\n",
        "\n",
        "base_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7utK44qzRR2k"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.unk_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pDl8xKmRR2k"
      },
      "outputs": [],
      "source": [
        "def generate_response(eval_prompt):\n",
        "    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        response = tokenizer.decode(base_model.generate(**model_input, max_new_tokens=512, repetition_penalty=1.15, pad_token_id=tokenizer.unk_token_id)[0], skip_special_tokens=False)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Wr28MfZPRR2k",
        "outputId": "db91400a-9a1e-40da-ff5f-60707d9936b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [1:05:02<00:00,  9.76s/it]\n"
          ]
        }
      ],
      "source": [
        "for el in tqdm(formatted_test_df):\n",
        "    response = generate_response(el[\"text\"].strip())\n",
        "    el[\"NF_triples\"] = response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kT4JK3koRR2l",
        "outputId": "c62785d1-9227-4dc7-a9f4-f7887564e6d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> [INST]Imagine being a Knowledge Graph constructor from unstructured text.\n",
            "Following the schema provided, extract all the triples you can find in the text.\n",
            "Make sure to mantain the formatting of the schema: \"(subject)-[PREDICATE]->(Object)\"\n",
            "\n",
            "Schema:\n",
            "(Legislator)-[REPRESENTS]->(StateCode)\n",
            "(Legislator)-[IS_MEMBER_OF]->(Party)\n",
            "(Legislator)-[ELECTED_TO]->(Body)\n",
            "(BillId)-[SPONSORED_BY]->(Legislator)\n",
            "(Legislator)-[VOTED_ON]->(BillId)\n",
            "(BillId)-[REFERRED_TO]->(Committee)\n",
            "(Legislator)-[SERVES_ON]->(Committee)\n",
            "(BillId)-[DEALS_WITH]->(Subject)\n",
            "\n",
            "Context:\n",
            "\"**Higher Education Enthusiasts Rejoice!**\n",
            "Calling all avid learners, knowledge seekers, and future leaders! Prepare yourselves for a tantalizing treat as we dive into the world of higher education.\n",
            "From the esteemed halls of hr2133-114, we unravel the complexities of both vocational and technical education. But that's not all! Also hailing from hr2133-114, brace yourselves for a journey through the realm of higher education.\n",
            "Embarking on a similar path, hr5310-114, hr3206-114, hr4371-114, hr386-114, and s674-114 all join hands, painting a vibrant tapestry of higher learning.\n",
            "But wait, there's more! hr4803-114 and hr3180-114 add their voices to the chorus, amplifying the importance of educational pursuits at the highest level. Finally, hr4098-114 completes the ensemble, emphasizing the transformative power of pursuing knowledge beyond the ordinary.\n",
            "So, whether you're a seasoned scholar or an aspiring newbie, let's embrace the endless possibilities that higher education has to offer. After all, the pursuit of knowledge is a lifelong adventure that empowers us to shape our futures and make a lasting impact on the world.\"[/INST]\n",
            "\n",
            "Extracted Triples:\n",
            "\n",
            "* (hr2133-114)-[REPRESENTS]->(vocational and technical education)\n",
            "* (hr2133-114)-[SERVES_ON]->(committee)\n",
            "* (hr5310-114)-[REPRESENTS]->(higher education)\n",
            "* (hr3206-114)-[REPRESENTS]->(higher education)\n",
            "* (hr4371-114)-[REPRESENTS]->(higher education)\n",
            "* (hr386-114)-[REPRESENTS]->(higher education)\n",
            "* (hr4803-114)-[REPRESENTS]->(educational pursuits)\n",
            "* (hr3180-114)-[REPRESENTS]->(educational pursuits)\n",
            "* (hr4098-114)-[REPRESENTS]->(pursuing knowledge beyond the ordinary)</s>\n"
          ]
        }
      ],
      "source": [
        "print(formatted_test_df[2][\"NF_triples\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGIKnH-gRR2l"
      },
      "outputs": [],
      "source": [
        "for el in formatted_test_df:\n",
        "    el[\"NF_triples\"] = el[\"NF_triples\"].split(\"Extracted Triples:\")[-1].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_O5pC4JRR2l",
        "outputId": "b751b0ff-4d53-4503-a6bf-8b6a1f6ba43f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* (hr2133-114)-[REPRESENTS]->(vocational and technical education)\n",
            "* (hr2133-114)-[SERVES_ON]->(committee)\n",
            "* (hr5310-114)-[REPRESENTS]->(higher education)\n",
            "* (hr3206-114)-[REPRESENTS]->(higher education)\n",
            "* (hr4371-114)-[REPRESENTS]->(higher education)\n",
            "* (hr386-114)-[REPRESENTS]->(higher education)\n",
            "* (hr4803-114)-[REPRESENTS]->(educational pursuits)\n",
            "* (hr3180-114)-[REPRESENTS]->(educational pursuits)\n",
            "* (hr4098-114)-[REPRESENTS]->(pursuing knowledge beyond the ordinary)</s>\n"
          ]
        }
      ],
      "source": [
        "print(formatted_test_df[2][\"NF_triples\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mac_-FK-RR2l"
      },
      "outputs": [],
      "source": [
        "# Save the responses\n",
        "import json\n",
        "json_str = json.dumps(formatted_test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWDtL3N4RR2m"
      },
      "outputs": [],
      "source": [
        "with open('NFresponses.json', 'w') as f:\n",
        "    json.dump(json_str, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUycj5TURR2m"
      },
      "source": [
        "# Generate responses (base model with few shot prompting)\n",
        "Generate the triples starting from the test set for Mistral 7B Instruct non-finetuned with 5 examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HO0QzV6LRR2m"
      },
      "outputs": [],
      "source": [
        "# importing the module\n",
        "import json\n",
        "\n",
        "# Opening JSON file\n",
        "with open('NFresponses.json') as json_file:\n",
        "    data = json.load(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN5O4eb7RR2m"
      },
      "outputs": [],
      "source": [
        "formatted_test_df = json.loads(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzcEPo3IRR2m",
        "outputId": "1daae80d-059d-443d-f7ce-6aeaf77b4617"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST]Imagine being a Knowledge Graph constructor from unstructured text.\n",
            "Following the schema provided, extract all the triples you can find in the text.\n",
            "Make sure to mantain the formatting of the schema: \"(subject)-[PREDICATE]->(Object)\"\n",
            "\n",
            "Schema:\n",
            "(Legislator)-[REPRESENTS]->(StateCode)\n",
            "(Legislator)-[IS_MEMBER_OF]->(Party)\n",
            "(Legislator)-[ELECTED_TO]->(Body)\n",
            "(BillId)-[SPONSORED_BY]->(Legislator)\n",
            "(Legislator)-[VOTED_ON]->(BillId)\n",
            "(BillId)-[REFERRED_TO]->(Committee)\n",
            "(Legislator)-[SERVES_ON]->(Committee)\n",
            "(BillId)-[DEALS_WITH]->(Subject)\n",
            "\n",
            "Context:\n",
            "\"Hi and welcome to our show about very interesting bills.\n",
            "Today's bill is the hr1818-114 bill. This bill was proposed by Ken Calvert, a Republican representative from California, and Grace Napolitano, a representative from California who is elected to the House. These two representatives are from the same state and share the sponsorship of the same bill.\"[/INST]\n",
            "\n",
            "Extracted Triples:\n"
          ]
        }
      ],
      "source": [
        "print(formatted_test_df[0][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avDtJO4fRR2m"
      },
      "source": [
        "## Build examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_m2k9CxRR2n",
        "outputId": "f84b760d-d604-4974-8d1c-0b55bbe9ebba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['0'],\n",
            "        num_rows: 2400\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['0'],\n",
            "        num_rows: 400\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "# Load Dataset\n",
        "data = load_dataset(\"Giardooo/KG_constructor\")\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO_QOjRTRR2n",
        "outputId": "449834b9-00a7-4ca8-8781-4c0e676a77a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST]Imagine being a Knowledge Graph constructor from unstructured text.\n",
            "Following the schema provided, extract all the triples you can find in the text.\n",
            "\n",
            "Schema:\n",
            "(Movie)-[IN_GENRE]->(Genre)\n",
            "(Person/Director/Actor)-[ACTED_IN]->(Movie)\n",
            "(Person/Director/Actor)-[DIRECTED]->(Movie)\n",
            "(User)-[RATED]->(Movie)\n",
            "\n",
            "Context:\n",
            "\"In a captivating mix of drama and mystery, filmmaker Gregg Araki left his mark with movies like \"Totally F***ed Up,\" \"White Bird in a Blizzard,\" and \"Kaboom.\" \"Totally F***ed Up,\" a poignant drama, featured talented actors such as James Duval, Susan Behshid, Roko Belic, and Jenee Gill in significant roles. Araki also delved into the enigmatic world of \"White Bird in a Blizzard,\" where Shailene Woodley showcased her acting prowess. Additionally, the versatile Thomas Dekker graced the screen in \"Kaboom,\" a film that garnered high praise from movie critic John Marshall, who rated both \"Kaboom\" and \"Totally F***ed Up\" with enthusiasm. Araki's unique storytelling and the stellar performances of the cast continue to captivate audiences and critics alike.\"[/INST]\n",
            "\n",
            "Extracted Triples:\n",
            "(Thomas Dekker) - [ACTED_IN] -> (Kaboom)\n",
            "(Gregg Araki) - [DIRECTED] -> (Totally F***ed Up)\n",
            "(Gregg Araki) - [DIRECTED] -> (White Bird in a Blizzard)\n",
            "(Gregg Araki) - [DIRECTED] -> (Kaboom)\n",
            "(Totally F***ed Up) - [IN_GENRE] -> (Drama)\n",
            "(James Duval) - [ACTED_IN] -> (Totally F***ed Up)\n",
            "(Susan Behshid) - [ACTED_IN] -> (Totally F***ed Up)\n",
            "(Roko Belic) - [ACTED_IN] -> (Totally F***ed Up)\n",
            "(Jenee Gill) - [ACTED_IN] -> (Totally F***ed Up)\n",
            "(John Marshall) - [RATED] -> (Kaboom)\n",
            "(John Marshall) - [RATED] -> (Totally F***ed Up)\n",
            "(White Bird in a Blizzard) - [IN_GENRE] -> (Drama)\n",
            "(Shailene Woodley) - [ACTED_IN] -> (White Bird in a Blizzard)</s>\n"
          ]
        }
      ],
      "source": [
        "print(data[\"train\"][1][\"0\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcNup-C_RR2o"
      },
      "outputs": [],
      "source": [
        "class_diz_count = {\"legis_graph\": 0, \"recommendations\": 0, \"recipes\": 0, \"listings\": 0}\n",
        "examples_diz = {\"legis_graph\": [], \"recommendations\": [], \"recipes\": [], \"listings\": []}\n",
        "\n",
        "for train_el in data[\"train\"]:\n",
        "    # Classify element\n",
        "    if \"DEALS_WITH\" in train_el[\"0\"]:\n",
        "        classification = \"legis_graph\"\n",
        "    elif \"ACTED_IN\" in train_el[\"0\"]:\n",
        "        classification = \"recommendations\"\n",
        "    elif \"COLLECTION\" in train_el[\"0\"]:\n",
        "        classification = \"recipes\"\n",
        "    elif \"IN_NEIGHBORHOOD\" in train_el[\"0\"]:\n",
        "        classification = \"listings\"\n",
        "    if class_diz_count[classification] == 5:\n",
        "        continue\n",
        "    class_diz_count[classification] += 1\n",
        "    example = \"Context:\\n\"\n",
        "    example += train_el[\"0\"].split(\"Context:\\n\")[-1][:-4]\n",
        "    example = example.replace(\"[/INST]\", \"\")\n",
        "    examples_diz[classification].append(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RPLAbo7RR2o"
      },
      "outputs": [],
      "source": [
        "examples_diz_text = {}\n",
        "for key in examples_diz.keys():\n",
        "    text = \"\"\n",
        "    for el in examples_diz[key]:\n",
        "        text += el\n",
        "        text += \"\\n-----------------\\n\"\n",
        "    examples_diz_text[key] = text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA_zutMgRR2p",
        "outputId": "1d4293a4-0530-4e25-c1f6-b6c26b4a2c03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context:\n",
            "\"Bills like the hr5833-114, hr4036-114, hr6311-114, s1910-114, hr2864-114, hr2910-114, and s2659-114 all deal with environmental regulatory procedures.\"\n",
            "\n",
            "Extracted Triples:\n",
            "(hr5833-114) - [DEALS_WITH] -> (Environmental regulatory procedures)\n",
            "(hr4036-114) - [DEALS_WITH] -> (Environmental regulatory procedures)\n",
            "(hr6311-114) - [DEALS_WITH] -> (Environmental regulatory procedures)\n",
            "(s1910-114) - [DEALS_WITH] -> (Environmental regulatory procedures)\n",
            "(hr2864-114) - [DEALS_WITH] -> (Environmental regulatory procedures)\n",
            "(hr2910-114) - [DEALS_WITH] -> (Environmental regulatory procedures)\n",
            "(s2659-114) - [DEALS_WITH] -> (Environmental regulatory procedures)\n",
            "-----------------\n",
            "Context:\n",
            "\"Hey guys, and welcome back to the \"Decoding Bills with Knowledge Graphs\" channel. Today, we're going to dive into the fascinating world of bills dealing with Members of Congress.\n",
            "Now, you might be wondering, what exactly do these bills entail? Well, that's where our trusty Knowledge Graph comes in. It tells us that bills like hr179-114, hr3077-114, hr4520-114, hr317-114, hr1381-114, s1810-114, s3120-114, and hr1032-114 all have one thing in common: they all deal with the esteemed Members of Congress.\n",
            "So there you have it - a concise walkthrough of bills related to Members of Congress. Stay tuned for more exciting Knowledge Graph adventures!\"\n",
            "\n",
            "Extracted Triples:\n",
            "(hr179-114) - [DEALS_WITH] -> (Members of Congress)\n",
            "(hr3077-114) - [DEALS_WITH] -> (Members of Congress)\n",
            "(hr4520-114) - [DEALS_WITH] -> (Members of Congress)\n",
            "(hr317-114) - [DEALS_WITH] -> (Members of Congress)\n",
            "(hr1381-114) - [DEALS_WITH] -> (Members of Congress)\n",
            "(s1810-114) - [DEALS_WITH] -> (Members of Congress)\n",
            "(s3120-114) - [DEALS_WITH] -> (Members of Congress)\n",
            "(hr1032-114) - [DEALS_WITH] -> (Members of Congress)\n",
            "-----------------\n",
            "Context:\n",
            "\"In today's episode, we'll be discussing the legislative contributions of Senator Martin Heinrich, a proud Democrat representing the state of New Mexico. Throughout his tenure in the Senate, Heinrich has played a significant role in sponsoring several bills, including S2540-114, S784-114, S1831-114, and S986-114, demonstrating his commitment to serving his constituents and shaping public policy.\"\n",
            "\n",
            "Extracted Triples:\n",
            "(s2540-114) - [SPONSORED_BY] -> (Martin Heinrich)\n",
            "(s2540-114) - [SPONSORED_BY] -> (Martin Heinrich)\n",
            "(Martin Heinrich) - [REPRESENTS] -> (NM)\n",
            "(Martin Heinrich) - [IS_MEMBER_OF] -> (Democrat)\n",
            "(Martin Heinrich) - [ELECTED_TO] -> (Senate)\n",
            "(s784-114) - [SPONSORED_BY] -> (Martin Heinrich)\n",
            "(s784-114) - [SPONSORED_BY] -> (Martin Heinrich)\n",
            "(s1831-114) - [SPONSORED_BY] -> (Martin Heinrich)\n",
            "(s1831-114) - [SPONSORED_BY] -> (Martin Heinrich)\n",
            "(s986-114) - [SPONSORED_BY] -> (Martin Heinrich)\n",
            "(s986-114) - [SPONSORED_BY] -> (Martin Heinrich)\n",
            "-----------------\n",
            "Context:\n",
            "\"\"Today, let's talk about some of the bills that have been introduced in the House of Representatives. Mike Quigley, representing Illinois, and Adam Kinzinger, also representing Illinois but belonging to the Republican Party, have co-sponsored Bill hr4514-114. It seems that Quigley and Kinzinger have a strong working relationship, as their names appear together on this bill multiple times.\n",
            "Kinzinger has also been busy sponsoring another bill, hr1867-114. Interestingly, both of these bills are sponsored solely by Kinzinger, showcasing his active involvement in the legislative process.\"\"\n",
            "\n",
            "Extracted Triples:\n",
            "(Mike Quigley ) - [REPRESENTS] -> (IL)\n",
            "(Mike Quigley ) - [ELECTED_TO] -> (House)\n",
            "(Adam Kinzinger) - [REPRESENTS] -> (IL)\n",
            "(Adam Kinzinger) - [IS_MEMBER_OF] -> (Republican)\n",
            "(Adam Kinzinger) - [ELECTED_TO] -> (House)\n",
            "(hr4514-114) - [SPONSORED_BY] -> (Adam Kinzinger)\n",
            "(hr4514-114) - [SPONSORED_BY] -> (Mike Quigley )\n",
            "(hr4514-114) - [SPONSORED_BY] -> (Mike Quigley )\n",
            "(hr4514-114) - [SPONSORED_BY] -> (Adam Kinzinger)\n",
            "(hr1867-114) - [SPONSORED_BY] -> (Adam Kinzinger)\n",
            "(hr1867-114) - [SPONSORED_BY] -> (Adam Kinzinger)\n",
            "-----------------\n",
            "Context:\n",
            "\"Hey guys, here we'll go over some bills that deal with different subjects in the District of Columbia. The first one we have is the Bill ID s2808-114 that deals with the District of Columbia, followed by the hr4338-114 that also deals with the District of Columbia as well as state and local government operations.\n",
            "Next, we have the Bills hr2804-114, hr5664-114, hr947-114, s437-114, and s494-114 that all deal with state and local government operations.\"\n",
            "\n",
            "Extracted Triples:\n",
            "(s2808-114) - [DEALS_WITH] -> (District of Columbia)\n",
            "(hr4338-114) - [DEALS_WITH] -> (District of Columbia)\n",
            "(hr4338-114) - [DEALS_WITH] -> (State and local government operations)\n",
            "(hr2804-114) - [DEALS_WITH] -> (State and local government operations)\n",
            "(hr5664-114) - [DEALS_WITH] -> (State and local government operations)\n",
            "(hr947-114) - [DEALS_WITH] -> (State and local government operations)\n",
            "(s437-114) - [DEALS_WITH] -> (State and local government operations)\n",
            "(s494-114) - [DEALS_WITH] -> (State and local government operations)\n",
            "-----------------\n"
          ]
        }
      ],
      "source": [
        "print(examples_diz_text[\"legis_graph\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oghbYeo0RR2p"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "eXf8S9oFRR2p",
        "outputId": "74524cac-a15b-42a0-d5c9-655625e1a099"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [00:00<00:00, 173193.10it/s]\n"
          ]
        }
      ],
      "source": [
        "for el in tqdm(formatted_test_df):\n",
        "    # Classify element\n",
        "    if \"DEALS_WITH\" in el[\"text\"]:\n",
        "        classification = \"legis_graph\"\n",
        "    elif \"ACTED_IN\" in el[\"text\"]:\n",
        "        classification = \"recommendations\"\n",
        "    elif \"COLLECTION\" in el[\"text\"]:\n",
        "        classification = \"recipes\"\n",
        "    elif \"IN_NEIGHBORHOOD\" in el[\"text\"]:\n",
        "        classification = \"listings\"\n",
        "    text = el[\"text\"].split(\"Context:\\n\")\n",
        "    new_text = text[0] + \"Here are some examples:\\n\" + examples_diz_text[classification] + \"\\nContext:\\n\" + text[-1]\n",
        "    el[\"FS_prompt\"] = new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZeUJQVlRR2p",
        "outputId": "071307a1-84e6-4f98-996d-446bec88889f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST]Imagine being a Knowledge Graph constructor from unstructured text.\n",
            "Following the schema provided, extract all the triples you can find in the text.\n",
            "Make sure to mantain the formatting of the schema: \"(subject)-[PREDICATE]->(Object)\"\n",
            "\n",
            "Schema:\n",
            "(Legislator)-[REPRESENTS]->(StateCode)\n",
            "(Legislator)-[IS_MEMBER_OF]->(Party)\n",
            "(Legislator)-[ELECTED_TO]->(Body)\n",
            "(BillId)-[SPONSORED_BY]->(Legislator)\n",
            "(Legislator)-[VOTED_ON]->(BillId)\n",
            "(BillId)-[REFERRED_TO]->(Committee)\n",
            "(Legislator)-[SERVES_ON]->(Committee)\n",
            "(BillId)-[DEALS_WITH]->(Subject)\n",
            "\n",
            "Here are some examples:\n",
            "Context:\n",
            "\"Bills like the hr5833-114, hr4036-114, hr6311-114, s1910-114, hr2864-114, hr2910-114, and s2659-114 all deal with environmental regulatory procedures.\"\n",
            "\n",
            "Extracted Triples:\n",
            "(hr5833-114) - [DEALS_WITH] -> (Environmental regulatory procedures)\n",
            "(hr4036-114) - [DEALS_WITH] -> (Environmental regulatory procedures)\n",
            "(hr6311-114) - [DEALS_WITH] -> (Environmental regulatory procedures)\n",
            "(s1910-114) - [DEALS_WITH] -> (Environmental regulatory procedures)\n",
            "(hr2864-114) - [DEALS_WITH] -> (Environmental regulatory procedures)\n",
            "(hr2910-114) - [DEALS_WITH] -> (Environmental regulatory procedures)\n",
            "(s2659-114) - [DEALS_WITH] -> (Environmental regulatory procedures)\n",
            "-----------------\n",
            "Context:\n",
            "\"Hey guys, and welcome back to the \"Decoding Bills with Knowledge Graphs\" channel. Today, we're going to dive into the fascinating world of bills dealing with Members of Congress.\n",
            "Now, you might be wondering, what exactly do these bills entail? Well, that's where our trusty Knowledge Graph comes in. It tells us that bills like hr179-114, hr3077-114, hr4520-114, hr317-114, hr1381-114, s1810-114, s3120-114, and hr1032-114 all have one thing in common: they all deal with the esteemed Members of Congress.\n",
            "So there you have it - a concise walkthrough of bills related to Members of Congress. Stay tuned for more exciting Knowledge Graph adventures!\"\n",
            "\n",
            "Extracted Triples:\n",
            "(hr179-114) - [DEALS_WITH] -> (Members of Congress)\n",
            "(hr3077-114) - [DEALS_WITH] -> (Members of Congress)\n",
            "(hr4520-114) - [DEALS_WITH] -> (Members of Congress)\n",
            "(hr317-114) - [DEALS_WITH] -> (Members of Congress)\n",
            "(hr1381-114) - [DEALS_WITH] -> (Members of Congress)\n",
            "(s1810-114) - [DEALS_WITH] -> (Members of Congress)\n",
            "(s3120-114) - [DEALS_WITH] -> (Members of Congress)\n",
            "(hr1032-114) - [DEALS_WITH] -> (Members of Congress)\n",
            "-----------------\n",
            "Context:\n",
            "\"In today's episode, we'll be discussing the legislative contributions of Senator Martin Heinrich, a proud Democrat representing the state of New Mexico. Throughout his tenure in the Senate, Heinrich has played a significant role in sponsoring several bills, including S2540-114, S784-114, S1831-114, and S986-114, demonstrating his commitment to serving his constituents and shaping public policy.\"\n",
            "\n",
            "Extracted Triples:\n",
            "(s2540-114) - [SPONSORED_BY] -> (Martin Heinrich)\n",
            "(s2540-114) - [SPONSORED_BY] -> (Martin Heinrich)\n",
            "(Martin Heinrich) - [REPRESENTS] -> (NM)\n",
            "(Martin Heinrich) - [IS_MEMBER_OF] -> (Democrat)\n",
            "(Martin Heinrich) - [ELECTED_TO] -> (Senate)\n",
            "(s784-114) - [SPONSORED_BY] -> (Martin Heinrich)\n",
            "(s784-114) - [SPONSORED_BY] -> (Martin Heinrich)\n",
            "(s1831-114) - [SPONSORED_BY] -> (Martin Heinrich)\n",
            "(s1831-114) - [SPONSORED_BY] -> (Martin Heinrich)\n",
            "(s986-114) - [SPONSORED_BY] -> (Martin Heinrich)\n",
            "(s986-114) - [SPONSORED_BY] -> (Martin Heinrich)\n",
            "-----------------\n",
            "Context:\n",
            "\"\"Today, let's talk about some of the bills that have been introduced in the House of Representatives. Mike Quigley, representing Illinois, and Adam Kinzinger, also representing Illinois but belonging to the Republican Party, have co-sponsored Bill hr4514-114. It seems that Quigley and Kinzinger have a strong working relationship, as their names appear together on this bill multiple times.\n",
            "Kinzinger has also been busy sponsoring another bill, hr1867-114. Interestingly, both of these bills are sponsored solely by Kinzinger, showcasing his active involvement in the legislative process.\"\"\n",
            "\n",
            "Extracted Triples:\n",
            "(Mike Quigley ) - [REPRESENTS] -> (IL)\n",
            "(Mike Quigley ) - [ELECTED_TO] -> (House)\n",
            "(Adam Kinzinger) - [REPRESENTS] -> (IL)\n",
            "(Adam Kinzinger) - [IS_MEMBER_OF] -> (Republican)\n",
            "(Adam Kinzinger) - [ELECTED_TO] -> (House)\n",
            "(hr4514-114) - [SPONSORED_BY] -> (Adam Kinzinger)\n",
            "(hr4514-114) - [SPONSORED_BY] -> (Mike Quigley )\n",
            "(hr4514-114) - [SPONSORED_BY] -> (Mike Quigley )\n",
            "(hr4514-114) - [SPONSORED_BY] -> (Adam Kinzinger)\n",
            "(hr1867-114) - [SPONSORED_BY] -> (Adam Kinzinger)\n",
            "(hr1867-114) - [SPONSORED_BY] -> (Adam Kinzinger)\n",
            "-----------------\n",
            "Context:\n",
            "\"Hey guys, here we'll go over some bills that deal with different subjects in the District of Columbia. The first one we have is the Bill ID s2808-114 that deals with the District of Columbia, followed by the hr4338-114 that also deals with the District of Columbia as well as state and local government operations.\n",
            "Next, we have the Bills hr2804-114, hr5664-114, hr947-114, s437-114, and s494-114 that all deal with state and local government operations.\"\n",
            "\n",
            "Extracted Triples:\n",
            "(s2808-114) - [DEALS_WITH] -> (District of Columbia)\n",
            "(hr4338-114) - [DEALS_WITH] -> (District of Columbia)\n",
            "(hr4338-114) - [DEALS_WITH] -> (State and local government operations)\n",
            "(hr2804-114) - [DEALS_WITH] -> (State and local government operations)\n",
            "(hr5664-114) - [DEALS_WITH] -> (State and local government operations)\n",
            "(hr947-114) - [DEALS_WITH] -> (State and local government operations)\n",
            "(s437-114) - [DEALS_WITH] -> (State and local government operations)\n",
            "(s494-114) - [DEALS_WITH] -> (State and local government operations)\n",
            "-----------------\n",
            "Context:\n",
            "\"Hi and welcome to our show about very interesting bills.\n",
            "Today's bill is the hr1818-114 bill. This bill was proposed by Ken Calvert, a Republican representative from California, and Grace Napolitano, a representative from California who is elected to the House. These two representatives are from the same state and share the sponsorship of the same bill.\"[/INST]\n",
            "\n",
            "Extracted Triples:\n"
          ]
        }
      ],
      "source": [
        "print(formatted_test_df[0][\"FS_prompt\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "gQjaw6vTRR2p",
        "outputId": "01b82d3e-6b89-464c-fe13-ffa6e1dd9e12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MistralForCausalLM(\n",
              "  (model): MistralModel(\n",
              "    (embed_tokens): Embedding(32000, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x MistralDecoderLayer(\n",
              "        (self_attn): MistralSdpaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): MistralRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): MistralMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Higher model_max_length\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    model_max_length=4096,\n",
        "    padding_side=\"left\")\n",
        "\n",
        "base_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btudcizWRR2z"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.unk_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "5OViJ8UiRR20",
        "outputId": "1b32e26c-e544-4510-e551-4836b6a615aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/400 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
            "100%|██████████| 400/400 [1:08:04<00:00, 10.21s/it]\n"
          ]
        }
      ],
      "source": [
        "for el in tqdm(formatted_test_df):\n",
        "    response = generate_response(el[\"FS_prompt\"].strip())\n",
        "    el[\"FS_triples\"] = response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsYATHLYRR20",
        "outputId": "ebd9196c-60a6-4607-8025-4e767ce7ecb7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [00:00<00:00, 317509.77it/s]\n"
          ]
        }
      ],
      "source": [
        "for el in tqdm(formatted_test_df):\n",
        "    el[\"FS_triples\"] = el[\"FS_triples\"].split(\"Extracted Triples:\\n\")[-1].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4pu_nRBRR20"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "json_str = json.dumps(formatted_test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FZDClHcRR20"
      },
      "outputs": [],
      "source": [
        "with open('FSresponses.json', 'w') as f:\n",
        "    json.dump(json_str, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2Bp55-lRR20"
      },
      "source": [
        "# Generate responses (finetuned model)\n",
        "Generate the triples starting from the test set for Mistral 7B Instruct Finetuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6K1RdQuRR20"
      },
      "outputs": [],
      "source": [
        "# importing the module\n",
        "import json\n",
        "\n",
        "# Opening JSON file\n",
        "with open('FSresponses.json') as json_file:\n",
        "    data = json.load(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2BfEJc1RR21"
      },
      "outputs": [],
      "source": [
        "formatted_test_df = json.loads(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37oMkKq2RR21",
        "outputId": "89f1c361-739c-46ee-e0fc-416e07c0b655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST]Imagine being a Knowledge Graph constructor from unstructured text.\n",
            "Following the schema provided, extract all the triples you can find in the text.\n",
            "Make sure to mantain the formatting of the schema: \"(subject)-[PREDICATE]->(Object)\"\n",
            "\n",
            "Schema:\n",
            "(Legislator)-[REPRESENTS]->(StateCode)\n",
            "(Legislator)-[IS_MEMBER_OF]->(Party)\n",
            "(Legislator)-[ELECTED_TO]->(Body)\n",
            "(BillId)-[SPONSORED_BY]->(Legislator)\n",
            "(Legislator)-[VOTED_ON]->(BillId)\n",
            "(BillId)-[REFERRED_TO]->(Committee)\n",
            "(Legislator)-[SERVES_ON]->(Committee)\n",
            "(BillId)-[DEALS_WITH]->(Subject)\n",
            "\n",
            "Context:\n",
            "\"Hi and welcome to our show about very interesting bills.\n",
            "Today's bill is the hr1818-114 bill. This bill was proposed by Ken Calvert, a Republican representative from California, and Grace Napolitano, a representative from California who is elected to the House. These two representatives are from the same state and share the sponsorship of the same bill.\"[/INST]\n",
            "\n",
            "Extracted Triples:\n"
          ]
        }
      ],
      "source": [
        "print(formatted_test_df[0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u27jdz4MRR21"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "ft_model = PeftModel.from_pretrained(base_model, \"Prova1/checkpoint-1000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "JKMPwcVARR21",
        "outputId": "d684583f-37ea-43cc-8353-7ab2ae930041"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32000, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralSdpaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): MistralRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=14336, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "      (lm_head): lora.Linear(\n",
              "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "        (lora_dropout): ModuleDict(\n",
              "          (default): Dropout(p=0.05, inplace=False)\n",
              "        )\n",
              "        (lora_A): ModuleDict(\n",
              "          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "        )\n",
              "        (lora_B): ModuleDict(\n",
              "          (default): Linear(in_features=8, out_features=32000, bias=False)\n",
              "        )\n",
              "        (lora_embedding_A): ParameterDict()\n",
              "        (lora_embedding_B): ParameterDict()\n",
              "        (lora_magnitude_vector): ModuleDict()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    model_max_length=1024,\n",
        "    padding_side=\"left\")\n",
        "\n",
        "ft_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBe-f2SfRR21"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.unk_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rs5pAV9uRR21"
      },
      "outputs": [],
      "source": [
        "def generate_response(eval_prompt):\n",
        "    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        response = tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=512, repetition_penalty=1.15, pad_token_id=tokenizer.unk_token_id)[0], skip_special_tokens=False)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "D-HrT7K3RR22",
        "outputId": "95bcc0c3-3e94-4d6a-93ff-1de45365d6c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/400 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
            "100%|██████████| 400/400 [1:49:11<00:00, 16.38s/it]\n"
          ]
        }
      ],
      "source": [
        "for el in tqdm(formatted_test_df):\n",
        "    response = generate_response(el[\"text\"].strip())\n",
        "    el[\"FT_triples\"] = response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3wlJp9tRR22",
        "outputId": "726dd79b-47cd-4acc-eced-ae1e1919d692"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [00:00<00:00, 317509.77it/s]\n"
          ]
        }
      ],
      "source": [
        "for el in tqdm(formatted_test_df):\n",
        "    el[\"FT_triples\"] = el[\"FT_triples\"].split(\"Extracted Triples:\\n\")[-1].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voRpMPNvRR22"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "json_str = json.dumps(formatted_test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_78Yj3AORR22"
      },
      "outputs": [],
      "source": [
        "with open('responses.json', 'w') as f:\n",
        "    json.dump(json_str, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a1vWA9kRR22"
      },
      "source": [
        "# External dbs test set\n",
        "Now we are going to repeat the same exact pipeline for the test_external_dbs.csv test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "lwTwF5EqRR23",
        "outputId": "8db02b2a-9e1e-42f6-d0f5-8fa7ef602f31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Downloading contourpy-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Downloading fonttools-4.53.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (162 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Downloading kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Downloading matplotlib-3.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m263.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.53.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m317.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
            "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.53.1 kiwisolver-1.4.5 matplotlib-3.9.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U datasets scipy ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "7b6a70d2d40b40549c6feb60ce2332e7"
          ]
        },
        "id": "lQvxdQEeRR23",
        "outputId": "4ed7739e-897e-4d2a-c374-745e794b8962"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b6a70d2d40b40549c6feb60ce2332e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "a1192dcab6a945a28caf4ac58efa37b9",
            "83b5493fd74347fe8f9bee52dfbea090",
            "7dddc17b133744c0836b1021ac6b8c12",
            "01b58f23fd7d437bb19e6a5dbf7fcf69",
            "f69cefb238e14db3b66919a928d6595c",
            "67563f3a97b845a5bd8967c54ab0d646",
            "7f8e8c0e7d864270912ecd779567679d"
          ]
        },
        "id": "cYuLAuvvRR23",
        "outputId": "100a2770-1b41-4cc3-e3d4-3b5a0942dc8b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1192dcab6a945a28caf4ac58efa37b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83b5493fd74347fe8f9bee52dfbea090",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7dddc17b133744c0836b1021ac6b8c12",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01b58f23fd7d437bb19e6a5dbf7fcf69",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f69cefb238e14db3b66919a928d6595c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67563f3a97b845a5bd8967c54ab0d646",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f8e8c0e7d864270912ecd779567679d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,  # Mistral, same as before\n",
        "    quantization_config=bnb_config,  # Same quantization config as before\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "011ccd97366a4facbea4bb388e4f5490",
            "537fa527cf2d4ca3a9912d67bcc4a749",
            "41ef24af80aa4cc5a500ed6aa9b62e14",
            "38d819eb7ee8412ca1fce0d05a78482f"
          ]
        },
        "id": "nNi7qI3RRR23",
        "outputId": "27c27f8c-197f-47c5-d19a-6251004fb2c1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "011ccd97366a4facbea4bb388e4f5490",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "537fa527cf2d4ca3a9912d67bcc4a749",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41ef24af80aa4cc5a500ed6aa9b62e14",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38d819eb7ee8412ca1fce0d05a78482f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    add_bos_token=False,\n",
        "    add_eos_token=False,\n",
        "    trust_remote_code=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BtnIa6SRR23"
      },
      "source": [
        "# Load Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG9sgI0zRR23"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rQpZY1FRR24"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"test_external_dbs.csv\")[\"0\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pr8hBwreRR24"
      },
      "outputs": [],
      "source": [
        "test_df = list(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLHMxCMARR24"
      },
      "outputs": [],
      "source": [
        "formatted_test_df = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AljZqb-ARR24"
      },
      "outputs": [],
      "source": [
        "sentence = \"\"\"Following the schema provided, extract all the triples you can find in the text.\n",
        "Make sure to mantain the formatting of the schema: \"(subject)-[PREDICATE]->(Object)\" \"\"\"\n",
        "sentence = sentence.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oInX4qh6RR24",
        "outputId": "020fc5dd-adf7-440c-fda4-fae35d323a71"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<s>[INST]Imagine being a Knowledge Graph constructor from unstructured text.\\nFollowing the schema provided, extract all the triples you can find in the text.\\n\\nSchema:\\n(Team)-[PARTICIPATED_IN]->(Tournament)\\n(Person)-[PLAYED_IN]->(Tournament)\\n(Person)-[REPRESENTS]->(Team)\\n\\nContext:\\n\"\"Did you know that several countries and teams have participated in various international tournaments? In France 2019, we saw Valentina Giacinti playing for her country, as well as Argentina and Japan participating. Additionally, China PR also took part in this tournament. But did you know that Argentina has also participated in USA 2003? And so did Japan! It\\'s amazing how these countries and teams come together to showcase their skills and compete against each other.\"\"[/INST]\\n\\nExtracted Triples:\\n(Valentina Giacinti) - [PLAYED_IN] -> (France 2019)\\n(Argentina) - [PARTICIPATED_IN] -> (France 2019)\\n(Argentina) - [PARTICIPATED_IN] -> (USA 2003)\\n(Japan) - [PARTICIPATED_IN] -> (France 2019)\\n(Japan) - [PARTICIPATED_IN] -> (USA 2003)\\n(China PR) - [PARTICIPATED_IN] -> (France 2019)\\n(China PR) - [PARTICIPATED_IN] -> (USA 2003)</s>'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrmrEel2RR25",
        "outputId": "d01bc0e0-a2fb-430a-8fdb-0848494fc3c6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 210/210 [00:00<00:00, 70441.77it/s]\n"
          ]
        }
      ],
      "source": [
        "for el in tqdm(test_df):\n",
        "    new_diz = {}\n",
        "    splitted_el = el.split(\"[/INST]\\n\\nExtracted Triples:\")\n",
        "    text = splitted_el[0][3:].strip()\n",
        "    text = text.split(\"Following the schema provided, extract all the triples you can find in the text.\")\n",
        "    text = sentence.join(text)\n",
        "    text += \"[/INST]\\n\\nExtracted Triples:\"\n",
        "    new_diz[\"text\"] = text\n",
        "    new_diz[\"triples\"] = splitted_el[1][:-4].strip()\n",
        "    formatted_test_df.append(new_diz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oyqq9yl7RR25",
        "outputId": "ea8f4cee-2a89-4744-8f69-04315a5e9470"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': '[INST]Imagine being a Knowledge Graph constructor from unstructured text.\\nFollowing the schema provided, extract all the triples you can find in the text.\\nMake sure to mantain the formatting of the schema: \"(subject)-[PREDICATE]->(Object)\"\\n\\nSchema:\\n(Team)-[PARTICIPATED_IN]->(Tournament)\\n(Person)-[PLAYED_IN]->(Tournament)\\n(Person)-[REPRESENTS]->(Team)\\n\\nContext:\\n\"\"Did you know that several countries and teams have participated in various international tournaments? In France 2019, we saw Valentina Giacinti playing for her country, as well as Argentina and Japan participating. Additionally, China PR also took part in this tournament. But did you know that Argentina has also participated in USA 2003? And so did Japan! It\\'s amazing how these countries and teams come together to showcase their skills and compete against each other.\"\"[/INST]\\n\\nExtracted Triples:',\n",
              " 'triples': '(Valentina Giacinti) - [PLAYED_IN] -> (France 2019)\\n(Argentina) - [PARTICIPATED_IN] -> (France 2019)\\n(Argentina) - [PARTICIPATED_IN] -> (USA 2003)\\n(Japan) - [PARTICIPATED_IN] -> (France 2019)\\n(Japan) - [PARTICIPATED_IN] -> (USA 2003)\\n(China PR) - [PARTICIPATED_IN] -> (France 2019)\\n(China PR) - [PARTICIPATED_IN] -> (USA 2003)'}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "formatted_test_df[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0_ztsicRR25"
      },
      "source": [
        "# Generate responses (base model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXQIbIRmRR25",
        "outputId": "c3c14ea9-f214-4255-cded-1c72c3c283fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MistralForCausalLM(\n",
              "  (model): MistralModel(\n",
              "    (embed_tokens): Embedding(32000, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x MistralDecoderLayer(\n",
              "        (self_attn): MistralSdpaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): MistralRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): MistralMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    model_max_length=1024,\n",
        "    padding_side=\"left\")\n",
        "\n",
        "base_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6NIOQPMRR25"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.unk_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2jROWcLRR26"
      },
      "outputs": [],
      "source": [
        "def generate_response(eval_prompt):\n",
        "    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        response = tokenizer.decode(base_model.generate(**model_input, max_new_tokens=512, repetition_penalty=1.15, pad_token_id=tokenizer.unk_token_id)[0], skip_special_tokens=False)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "72JVMomSRR26",
        "outputId": "25ca3c58-4c53-4396-fd4d-cd9ce9a182fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/210 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
            "100%|██████████| 210/210 [43:34<00:00, 12.45s/it]\n"
          ]
        }
      ],
      "source": [
        "for el in tqdm(formatted_test_df):\n",
        "    response = generate_response(el[\"text\"].strip())\n",
        "    el[\"NF_triples\"] = response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anO3xtNgRR26",
        "outputId": "d374e20f-ba4f-4e6f-bc42-bc20571ab518"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> [INST]Imagine being a Knowledge Graph constructor from unstructured text.\n",
            "Following the schema provided, extract all the triples you can find in the text.\n",
            "Make sure to mantain the formatting of the schema: \"(subject)-[PREDICATE]->(Object)\"\n",
            "\n",
            "Schema:\n",
            "(Team)-[PARTICIPATED_IN]->(Tournament)\n",
            "(Person)-[PLAYED_IN]->(Tournament)\n",
            "(Person)-[REPRESENTS]->(Team)\n",
            "\n",
            "Context:\n",
            "\"\"Sophie Schmidt, a Canadian soccer player, played for Ghana in the China 2007 tournament. She was part of the Canada team that also participated in the same tournament. The China PR and Brazil teams were also present at the event.\"\"[/INST]\n",
            "\n",
            "Extracted Triples:\n",
            "\n",
            "1. (Sophie Schmidt)-[PLAYED\\_IN]->(China 2007 tournament)\n",
            "2. (Canada team)-[PARTICIPATED\\_IN]->(China 2007 tournament)\n",
            "3. (Ghana team)-[PARTICIPATED\\_IN]->(China 2007 tournament)\n",
            "4. (Brazil team)-[PARTICIPATED\\_IN]->(China 2007 tournament)</s>\n"
          ]
        }
      ],
      "source": [
        "print(formatted_test_df[2][\"NF_triples\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-jG9z1XRR26",
        "outputId": "9c61b1a6-c842-4f4f-fa8a-42a5133bb185"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "210"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(formatted_test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hD77aroURR26",
        "outputId": "940611ca-45db-47b7-fd69-04fd791dd0b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': '[INST]Imagine being a Knowledge Graph constructor from unstructured text.\\nFollowing the schema provided, extract all the triples you can find in the text.\\nMake sure to mantain the formatting of the schema: \"(subject)-[PREDICATE]->(Object)\"\\n\\nSchema:\\n(House)-[ATTACKER, DEFENDER]->(Battle)\\n(House)-[ATTACKER, DEFENDER]->(Battle)\\n(Battle, Location)-[IS_IN]->(Location, Region)\\n(Knight, King)-[DEFENDER_COMMANDER,ATTACKER_COMMANDER]->(Battle)\\n(Knight, King)-[DEFENDER_COMMANDER,ATTACKER_COMMANDER]->(Battle)\\n(King)-[DEFENDER_KING, ATTACKER_KING]->(Battle)\\n(King)-[DEFENDER_KING, ATTACKER_KING]->(Battle)\\n(Person,Knight, King)-[BELONGS_TO]->(House)\\n(Person,Knight, King)-[HAS_STATUS]->(Status)\\n(Person,Knight,King)-[APPEARED_IN, DIED_IN]->(Book)\\n(Person,Knight,King)-[APPEARED_IN, DIED_IN]->(Book)\\n(Person, King, Knight)-[MEMBER_OF_CULTURE]->(Culture)\\n(Person, Knight, King)-[INTERACTS]->(Person, Knight, King)\\n(Person, Knight, King)<-[RELATED_TO]->(Person, Knight, King)\\n\\nContext:\\n\"In the epic battle known as the Storm of Swords, several key figures were present. Among them was Androw Frey, who appeared alongside his fellow knights and kings at this pivotal moment in history. Another notable figure was Barth, who also made an appearance in the same event. Both men later returned for the Feast for Crows and Dance with Dragons. It\\'s worth noting that both Androw Frey and Barth belonged to the Faith of the Seven. Meanwhile, Stafford Lannister and Robb Stark were also present in the Storm of Swords, with the latter unfortunately meeting his end during the conflict. Despite their differences, these individuals all played important roles in shaping the course of events leading up to the current day.\"[/INST]\\n\\nExtracted Triples:',\n",
              " 'triples': '(Androw Frey) - [APPEARED_IN] -> (Storm of swords)\\n(Androw Frey) - [APPEARED_IN] -> (Feast for crows)\\n(Barth) - [APPEARED_IN] -> (Storm of swords)\\n(Barth) - [APPEARED_IN] -> (Feast for crows)\\n(Barth) - [APPEARED_IN] -> (Dance with dragons)\\n(Barth) - [BELONGS_TO] -> (Faith of the Seven)\\n(Stafford Lannister) - [APPEARED_IN] -> (Storm of swords)\\n(Stafford Lannister) - [APPEARED_IN] -> (Feast for crows)\\n(Stafford Lannister) - [APPEARED_IN] -> (Dance with dragons)\\n(Robb Stark) - [APPEARED_IN] -> (Storm of swords)\\n(Robb Stark) - [DIED_IN] -> (Storm of swords)\\n(Robb Stark) - [INTERACTS] -> (Stafford Lannister)',\n",
              " 'NF_triples': '<s> [INST]Imagine being a Knowledge Graph constructor from unstructured text.\\nFollowing the schema provided, extract all the triples you can find in the text.\\nMake sure to mantain the formatting of the schema: \"(subject)-[PREDICATE]->(Object)\"\\n\\nSchema:\\n(House)-[ATTACKER, DEFENDER]->(Battle)\\n(House)-[ATTACKER, DEFENDER]->(Battle)\\n(Battle, Location)-[IS_IN]->(Location, Region)\\n(Knight, King)-[DEFENDER_COMMANDER,ATTACKER_COMMANDER]->(Battle)\\n(Knight, King)-[DEFENDER_COMMANDER,ATTACKER_COMMANDER]->(Battle)\\n(King)-[DEFENDER_KING, ATTACKER_KING]->(Battle)\\n(King)-[DEFENDER_KING, ATTACKER_KING]->(Battle)\\n(Person,Knight, King)-[BELONGS_TO]->(House)\\n(Person,Knight, King)-[HAS_STATUS]->(Status)\\n(Person,Knight,King)-[APPEARED_IN, DIED_IN]->(Book)\\n(Person,Knight,King)-[APPEARED_IN, DIED_IN]->(Book)\\n(Person, King, Knight)-[MEMBER_OF_CULTURE]->(Culture)\\n(Person, Knight, King)-[INTERACTS]->(Person, Knight, King)\\n(Person, Knight, King)<-[RELATED_TO]->(Person, Knight, King)\\n\\nContext:\\n\"In the epic battle known as the Storm of Swords, several key figures were present. Among them was Androw Frey, who appeared alongside his fellow knights and kings at this pivotal moment in history. Another notable figure was Barth, who also made an appearance in the same event. Both men later returned for the Feast for Crows and Dance with Dragons. It\\'s worth noting that both Androw Frey and Barth belonged to the Faith of the Seven. Meanwhile, Stafford Lannister and Robb Stark were also present in the Storm of Swords, with the latter unfortunately meeting his end during the conflict. Despite their differences, these individuals all played important roles in shaping the course of events leading up to the current day.\"[/INST]\\n\\nExtracted Triples:\\n\\n* (Androw Frey)-[ATTACKER]->(Stafford Lannister)\\n* (Androw Frey)-[DEFENDER]->(Robb Stark)\\n* (Barath)-[ATTACKER]->(Unknown)\\n* (Barath)-[DEFENDER]->(Unknown)\\n* (Stafford Lannister)-[ATTACKER]->(Robb Stark)\\n* (Robb Stark)-[DEFENDER]->(Stafford Lannister)\\n* (Androw Frey)-[ATTACKER]->(Unknown)\\n* (Androw Frey)-[DEFENDER]->(Unknown)\\n* (Barath)-[ATTACKER]->(Unknown)\\n* (Barath)-[DEFENDER]->(Unknown)\\n* (Stafford Lannister)-[ATTACKER]->(Robb Stark)\\n* (Robb Stark)-[DEFENDER]->(Stafford Lannister)\\n* (Androw Frey)-[ATTACKER]->(Unknown)\\n* (Androw Frey)-[DEFENDER]->(Unknown)\\n* (Barath)-[ATTACKER]->(Unknown)\\n* (Barath)-[DEFENDER]->(Unknown)\\n* (Stafford Lannister)-[ATTACKER]->(Robb Stark)\\n* (Robb Stark)-[DEFENDER]->(Stafford Lannister)</s>'}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "formatted_test_df[105]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zznq5b30RR27"
      },
      "outputs": [],
      "source": [
        "# We save 10 examples for the Few Shot prompting\n",
        "examples= {\"graph-data-science\": formatted_test_df[-5:], \"wwc2019\": formatted_test_df[:5]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2he12bMRR27"
      },
      "outputs": [],
      "source": [
        "# We mantain in the list the other 100 examples\n",
        "formatted_test_df = formatted_test_df[5:-5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivWBVEzXRR27",
        "outputId": "dc1cfca4-6def-4536-ba3b-d33179a2523a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(formatted_test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9YL_ERTRR27"
      },
      "outputs": [],
      "source": [
        "formatted_test_df2 = formatted_test_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ6HT18uRR27"
      },
      "outputs": [],
      "source": [
        "for el in formatted_test_df:\n",
        "    el[\"NF_triples\"] = el[\"NF_triples\"].split(\"Extracted Triples:\")[-1].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Arg1aZG8RR28",
        "outputId": "22219910-c8df-49b6-b200-9bcdfcc74127"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* (Gudrun Gottschlich)-[PLAYED_IN]->(China PR)\n",
            "* (Japan)-[PARTICIPATED_IN]->(Germany)\n",
            "* (Japan)-[PARTICIPATED_IN]->(France)\n",
            "* (Japan)-[PARTICIPATED_IN]->(China PR)\n",
            "* (Japan)-[PARTICIPATED_IN]->(USA)\n",
            "* (Japan)-[PARTICIPATED_IN]->(Sweden)\n",
            "* (Japan)-[PARTICIPATED_IN]->(Canada)</s>\n"
          ]
        }
      ],
      "source": [
        "print(formatted_test_df[2][\"NF_triples\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OV6qHZiORR28"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "json_str = json.dumps(formatted_test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5klYICHRR28"
      },
      "outputs": [],
      "source": [
        "with open('NFresponses.json', 'w') as f:\n",
        "    json.dump(json_str, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mjo8uacRR28"
      },
      "source": [
        "# Generate responses (few shot prompting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH_KTD9DRR28"
      },
      "outputs": [],
      "source": [
        "# importing the module\n",
        "import json\n",
        "\n",
        "# Opening JSON file\n",
        "with open('NFresponses.json') as json_file:\n",
        "    data = json.load(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7sbck9GRR29"
      },
      "outputs": [],
      "source": [
        "formatted_test_df = json.loads(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKmmmogARR29",
        "outputId": "3400b43d-43b8-46ea-8b43-e03aff984205"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST]Imagine being a Knowledge Graph constructor from unstructured text.\n",
            "Following the schema provided, extract all the triples you can find in the text.\n",
            "Make sure to mantain the formatting of the schema: \"(subject)-[PREDICATE]->(Object)\"\n",
            "\n",
            "Schema:\n",
            "(Team)-[PARTICIPATED_IN]->(Tournament)\n",
            "(Person)-[PLAYED_IN]->(Tournament)\n",
            "(Person)-[REPRESENTS]->(Team)\n",
            "\n",
            "Context:\n",
            "\"France 2019 was a soccer tournament held in France, which saw participation from various teams around the world. The China PR team, represented by Li Ying, participated in the tournament. Additionally, Argentina, Australia, Brazil, Cameroon, Canada, and Chile also took part in the event.\"[/INST]\n",
            "\n",
            "Extracted Triples:\n"
          ]
        }
      ],
      "source": [
        "print(formatted_test_df[0][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rOQsuvxRR29"
      },
      "source": [
        "## Build examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdpNEnsmRR29"
      },
      "outputs": [],
      "source": [
        "examples_diz_text = {}\n",
        "for key in examples.keys():\n",
        "    text = \"\"\n",
        "    for el in examples[key]:\n",
        "        text += \"Context:\\n\" + el[\"text\"].split(\"Context:\\n\")[-1] + \"\\n\" + el[\"triples\"]\n",
        "        text += \"\\n-----------------\\n\"\n",
        "    examples_diz_text[key] = text.replace(\"[INST]\", \"\").replace(\"[/INST]\", \"\").strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kh0gitLMRR29",
        "outputId": "c1c73986-7447-402c-9bee-50132ce94752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context:\n",
            "\"\"Feast for Crows\" is a novel written by George R.R Martin and published in 2013. The book is set in the fictional world of Westeros and follows the events of the Red Wedding, which marked the end of the War of the Five Kings. In this novel, several characters appear, including Robin, Meg, Jeyne Darry, Marianne Vance, Malwyn Frey, and Raynard. These characters are all present at the Feast for Crows, where they witness important events and interact with each other.\"\n",
            "\n",
            "Extracted Triples:\n",
            "(Robin) - [APPEARED_IN] -> (Feast for crows)\n",
            "(Meg) - [APPEARED_IN] -> (Feast for crows)\n",
            "(Jeyne Darry) - [APPEARED_IN] -> (Feast for crows)\n",
            "(Marianne Vance) - [APPEARED_IN] -> (Feast for crows)\n",
            "(Malwyn Frey) - [APPEARED_IN] -> (Feast for crows)\n",
            "(Raynard) - [APPEARED_IN] -> (Feast for crows)\n",
            "-----------------\n",
            "Context:\n",
            "\"\"Welcome back to our channel where we dive into the world of Game of Thrones and explore its intricate web of characters and events. Today, we'll take a closer look at some key figures who appeared in two major battles of the series: Storm of Swords and Feast for Crows.\n",
            "First up, we have Joth Quickbow, who made his appearance in the Battle of Storm of Swords. He was a skilled archer who played a crucial role in defending King's Landing against the invading forces of House Bolton.\n",
            "Next, we have Barth, another important character who fought in both battles. He was a member of House Tyrell and served as Lord Petyr Baelish's right-hand man. In the Battle of Storm of Swords, he led the charge against the Lannister army and helped turn the tide of the battle.\n",
            "Meg also made her mark in the Battle of Feast for Crows, where she played a pivotal role in the capture of Harrenhal by House Targaryen. She was a fierce warrior who proved herself to be a valuable asset to the Targaryen forces.\n",
            "Jeyne Darry was another key figure who appeared in both battles. She was a loyal servant of House Stark who played a critical role in securing the safety of Winterfell during the Battle of Storm of Swords. Later, she helped negotiate the surrender of House Bolton during the Battle of Feast for Crows.\n",
            "Finally, we have Marianne Vance, who made her debut in the Battle of Feast for Crows. She was a member of House Martell who played a vital role in the siege of Harrenhal. Her skills as a diplomat and negotiator were instrumental in securing the surrender of House Bolton.\n",
            "And lastly, Malwyn Frey, who appeared in both battles. He was a treacherous character who betrayed House Stark during the Battle of Storm of Swords. However, he managed to survive the battle and later played a key role in the Battle of Feast for Crows, where he helped secure the victory for House Lannister.\"\"\n",
            "\n",
            "Extracted Triples:\n",
            "(Joth Quickbow) - [APPEARED_IN] -> (Storm of swords)\n",
            "(Barth) - [APPEARED_IN] -> (Storm of swords)\n",
            "(Barth) - [APPEARED_IN] -> (Feast for crows)\n",
            "(Meg) - [APPEARED_IN] -> (Feast for crows)\n",
            "(Jeyne Darry) - [APPEARED_IN] -> (Feast for crows)\n",
            "(Jeyne Darry) - [APPEARED_IN] -> (Storm of swords)\n",
            "(Marianne Vance) - [APPEARED_IN] -> (Feast for crows)\n",
            "(Marianne Vance) - [APPEARED_IN] -> (Storm of swords)\n",
            "(Malwyn Frey) - [APPEARED_IN] -> (Feast for crows)\n",
            "(Malwyn Frey) - [APPEARED_IN] -> (Storm of swords)\n",
            "-----------------\n",
            "Context:\n",
            "\"Hello and welcome to our podcast where we delve into the world of Game of Thrones. Today, we'll be discussing the characters who appeared in Feast for Crows and their affiliations with the Brotherhood Without Banners.\n",
            "First up, Symon Hollard, Meg, Likely Luke, Jeyne Heddle, and Melly all made appearances in Feast for Crows. These individuals were all members of the Brotherhood Without Banners, which was a group of knights who fought against tyranny and oppression.\n",
            "It's worth noting that while these characters may have had different backgrounds and motivations, they all shared a common goal of protecting those who could not protect themselves. The Brotherhood Without Banners was known for its bravery and dedication to justice, making it one of the most respected groups in Westeros.\n",
            "In conclusion, the characters we discussed today all played important roles in the fight against tyranny and oppression. Their affiliation with the Brotherhood Without Banners highlights the importance of standing up for what is right, even in the face of adversity.\"\n",
            "\n",
            "Extracted Triples:\n",
            "(Symon Hollard) - [APPEARED_IN] -> (Feast for crows)\n",
            "(Meg) - [APPEARED_IN] -> (Feast for crows)\n",
            "(Meg) - [BELONGS_TO] -> (Brotherhood Without Banners)\n",
            "(Likely Luke) - [APPEARED_IN] -> (Feast for crows)\n",
            "(Likely Luke) - [BELONGS_TO] -> (Brotherhood Without Banners)\n",
            "(Jeyne Heddle) - [APPEARED_IN] -> (Feast for crows)\n",
            "(Jeyne Heddle) - [BELONGS_TO] -> (Brotherhood Without Banners)\n",
            "(Melly) - [BELONGS_TO] -> (Brotherhood Without Banners)\n",
            "-----------------\n",
            "Context:\n",
            "\"Hello and welcome to our podcast where we'll be discussing the world of Dance with Dragons. Today, we have four special guests joining us: Shiera Crakehall, Renly Baratheon, Pyat Pree, and Cragorn. Let's start by talking about Shiera Crakehall. She belonged to the House of Crakehall and appeared in Dance with Dragons. Unfortunately, she died in the same book. Next up, we have Renly Baratheon who also appeared in Dance with Dragons and was a member of House Lannister. Lastly, we have Pyat Pree, who was a mysterious figure in the series and appeared in Dance with Dragons as well. And finally, we have Cragorn, a dragon who played a significant role in the story. Thank you for tuning in and we hope you enjoyed this discussion.\"\n",
            "\n",
            "Extracted Triples:\n",
            "(Shiera Crakehall) - [APPEARED_IN] -> (Dance with dragons)\n",
            "(Shiera Crakehall) - [BELONGS_TO] -> (Crakehall)\n",
            "(Renly Baratheon) - [APPEARED_IN] -> (Dance with dragons)\n",
            "(Pyat Pree) - [APPEARED_IN] -> (Dance with dragons)\n",
            "(Cragorn) - [APPEARED_IN] -> (Dance with dragons)\n",
            "-----------------\n",
            "Context:\n",
            "\"\"Hey guys! I just wanted to share some interesting information about the Osgrey family. It turns out that Addam Osgrey, Wat, Will, Squinty Jeyne, Rob, and Perwyn Osgrey are all members of the same house. That's right, they all belong to the House of Osgrey! This means that they are all related by blood and have a shared history and culture. If you're interested in learning more about this fascinating family, be sure to check out our knowledge graph for more details.\"\"\n",
            "\n",
            "Extracted Triples:\n",
            "(Addam Osgrey) - [BELONGS_TO] -> (Osgrey)\n",
            "(Wat ) - [BELONGS_TO] -> (Osgrey)\n",
            "(Will ) - [BELONGS_TO] -> (Osgrey)\n",
            "(Squinty Jeyne) - [BELONGS_TO] -> (Osgrey)\n",
            "(Rob) - [BELONGS_TO] -> (Osgrey)\n",
            "(Perwyn Osgrey) - [BELONGS_TO] -> (Osgrey)\n",
            "-----------------\n"
          ]
        }
      ],
      "source": [
        "print(examples_diz_text[\"graph-data-science\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_WjZ3QbRR29"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "d8PIq-4iRR2-",
        "outputId": "3f6e7f03-6be4-4348-ae6c-14cb6bc6380f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:00<00:00, 198499.95it/s]\n"
          ]
        }
      ],
      "source": [
        "for el in tqdm(formatted_test_df):\n",
        "    # Classify element\n",
        "    if \"DEFENDER_KING\" in el[\"text\"]:\n",
        "        classification = \"graph-data-science\"\n",
        "    elif \"PLAYED_IN\" in el[\"text\"]:\n",
        "        classification = \"wwc2019\"\n",
        "    text = el[\"text\"].split(\"Context:\\n\")\n",
        "    new_text = text[0] + \"Here are some examples:\\n\" + examples_diz_text[classification] + \"\\nContext:\\n\" + text[-1]\n",
        "    el[\"FS_prompt\"] = new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-DX0FszRR2-",
        "outputId": "538ebc18-e7cf-4cd8-ae05-fe392e10404a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST]Imagine being a Knowledge Graph constructor from unstructured text.\n",
            "Following the schema provided, extract all the triples you can find in the text.\n",
            "Make sure to mantain the formatting of the schema: \"(subject)-[PREDICATE]->(Object)\"\n",
            "\n",
            "Schema:\n",
            "(Team)-[PARTICIPATED_IN]->(Tournament)\n",
            "(Person)-[PLAYED_IN]->(Tournament)\n",
            "(Person)-[REPRESENTS]->(Team)\n",
            "\n",
            "Here are some examples:\n",
            "Context:\n",
            "\"\"Did you know that several countries and teams have participated in various international tournaments? In France 2019, we saw Valentina Giacinti playing for her country, as well as Argentina and Japan participating. Additionally, China PR also took part in this tournament. But did you know that Argentina has also participated in USA 2003? And so did Japan! It's amazing how these countries and teams come together to showcase their skills and compete against each other.\"\"\n",
            "\n",
            "Extracted Triples:\n",
            "(Valentina Giacinti) - [PLAYED_IN] -> (France 2019)\n",
            "(Argentina) - [PARTICIPATED_IN] -> (France 2019)\n",
            "(Argentina) - [PARTICIPATED_IN] -> (USA 2003)\n",
            "(Japan) - [PARTICIPATED_IN] -> (France 2019)\n",
            "(Japan) - [PARTICIPATED_IN] -> (USA 2003)\n",
            "(China PR) - [PARTICIPATED_IN] -> (France 2019)\n",
            "(China PR) - [PARTICIPATED_IN] -> (USA 2003)\n",
            "-----------------\n",
            "Context:\n",
            "\"Hello and welcome to our special report on the Germany 2011 tournament. We have gathered some interesting information about the participants and their teams. Let's start with Therese Sjogran, who played for Equatorial Guinea in the tournament. She was joined by Miriam, Chinasa, and Christelle, all of whom also represented Equatorial Guinea. It's fascinating how these players came together to compete in such a prestigious event.\"\n",
            "\n",
            "Extracted Triples:\n",
            "(Therese Sjogran) - [PLAYED_IN] -> (Germany 2011)\n",
            "(Equatorial Guinea) - [PARTICIPATED_IN] -> (Germany 2011)\n",
            "(Miriam) - [PLAYED_IN] -> (Germany 2011)\n",
            "(Miriam) - [REPRESENTS] -> (Equatorial Guinea)\n",
            "(Chinasa) - [PLAYED_IN] -> (Germany 2011)\n",
            "(Chinasa) - [REPRESENTS] -> (Equatorial Guinea)\n",
            "(Christelle) - [PLAYED_IN] -> (Germany 2011)\n",
            "(Christelle) - [REPRESENTS] -> (Equatorial Guinea)\n",
            "-----------------\n",
            "Context:\n",
            "\"\"Sophie Schmidt, a Canadian soccer player, played for Ghana in the China 2007 tournament. She was part of the Canada team that also participated in the same tournament. The China PR and Brazil teams were also present at the event.\"\"\n",
            "\n",
            "Extracted Triples:\n",
            "(Sophie Schmidt) - [PLAYED_IN] -> (China 2007)\n",
            "(Sophie Schmidt) - [REPRESENTS] -> (Canada)\n",
            "(Ghana) - [PARTICIPATED_IN] -> (China 2007)\n",
            "(China PR) - [PARTICIPATED_IN] -> (China 2007)\n",
            "(Canada) - [PARTICIPATED_IN] -> (China 2007)\n",
            "(Brazil) - [PARTICIPATED_IN] -> (China 2007)\n",
            "-----------------\n",
            "Context:\n",
            "\"Hello and welcome to our special podcast episode where we'll be discussing the incredible achievements of the Russian women's volleyball team at the USA 1999 tournament. We have with us three key players who played a crucial role in Russia's success at the event: Elena Lissacheva, Natalia Karasseva, and Olga Karasseva.\n",
            "First up, let's talk about Elena Lissacheva. She was one of the most talented players on the court during the tournament and represented Russia with pride. Her skills were unmatched, and she quickly became a fan favorite.\n",
            "Next, we have Natalia Karasseva, who also played a pivotal role in Russia's victory. She was known for her powerful hits and impressive serves, which helped lead Russia to many victories.\n",
            "Lastly, we can't forget about Olga Karasseva, who was another standout player on the team. She was a true leader and inspired her teammates with her determination and passion for the game.\n",
            "All three of these players had a significant impact on Russia's success at the USA 1999 tournament, and their legacies continue to inspire young athletes today. Thank you for joining us on this journey through history, and we hope you enjoyed learning more about these amazing athletes.\"\n",
            "\n",
            "Extracted Triples:\n",
            "(Elena Lissacheva) - [REPRESENTS] -> (Russia)\n",
            "(Russia) - [PARTICIPATED_IN] -> (USA 1999)\n",
            "(Natalia Karasseva) - [PLAYED_IN] -> (USA 1999)\n",
            "(Natalia Karasseva) - [REPRESENTS] -> (Russia)\n",
            "(Olga Karasseva) - [PLAYED_IN] -> (USA 1999)\n",
            "(Olga Karasseva) - [REPRESENTS] -> (Russia)\n",
            "(Natalia Filippova) - [PLAYED_IN] -> (USA 1999)\n",
            "(Natalia Filippova) - [REPRESENTS] -> (Russia)\n",
            "-----------------\n",
            "Context:\n",
            "\"\"In the France 2019 tournament, Andile Dlamini played for his team, while Argentina and Japan also participated. In the USA 2003 tournament, both Argentina and Japan were present as well, along with China PR, Sweden, France, Norway, and the host country itself.\"\"\n",
            "\n",
            "Extracted Triples:\n",
            "(Andile Dlamini) - [PLAYED_IN] -> (France 2019)\n",
            "(Argentina) - [PARTICIPATED_IN] -> (France 2019)\n",
            "(Argentina) - [PARTICIPATED_IN] -> (USA 2003)\n",
            "(Japan) - [PARTICIPATED_IN] -> (France 2019)\n",
            "(Japan) - [PARTICIPATED_IN] -> (USA 2003)\n",
            "(China PR) - [PARTICIPATED_IN] -> (France 2019)\n",
            "(China PR) - [PARTICIPATED_IN] -> (USA 2003)\n",
            "(Sweden) - [PARTICIPATED_IN] -> (USA 2003)\n",
            "(Sweden) - [PARTICIPATED_IN] -> (France 2019)\n",
            "(France) - [PARTICIPATED_IN] -> (France 2019)\n",
            "(France) - [PARTICIPATED_IN] -> (USA 2003)\n",
            "(Norway) - [PARTICIPATED_IN] -> (USA 2003)\n",
            "(Norway) - [PARTICIPATED_IN] -> (France 2019)\n",
            "-----------------\n",
            "Context:\n",
            "\"France 2019 was a soccer tournament held in France, which saw participation from various teams around the world. The China PR team, represented by Li Ying, participated in the tournament. Additionally, Argentina, Australia, Brazil, Cameroon, Canada, and Chile also took part in the event.\"[/INST]\n",
            "\n",
            "Extracted Triples:\n"
          ]
        }
      ],
      "source": [
        "print(formatted_test_df[0][\"FS_prompt\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "-ns7bkC8RR2-",
        "outputId": "9b0ab973-bf3d-4373-c861-33c9100b5d29"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MistralForCausalLM(\n",
              "  (model): MistralModel(\n",
              "    (embed_tokens): Embedding(32000, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x MistralDecoderLayer(\n",
              "        (self_attn): MistralSdpaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): MistralRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): MistralMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Higher model_max_length\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    model_max_length=4096,\n",
        "    padding_side=\"left\")\n",
        "\n",
        "base_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqCaLD6ARR2-"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.unk_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "N96gkk4GRR2-",
        "outputId": "313db55d-5e7a-4c03-9efe-20cdfa88d28e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [28:38<00:00,  8.59s/it]\n"
          ]
        }
      ],
      "source": [
        "for el in tqdm(formatted_test_df):\n",
        "    response = generate_response(el[\"FS_prompt\"].strip())\n",
        "    el[\"FS_triples\"] = response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ4sQQu3RR2-",
        "outputId": "ee5ea077-c008-4c2e-b629-000d812f9d78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:00<00:00, 126965.46it/s]\n"
          ]
        }
      ],
      "source": [
        "for el in tqdm(formatted_test_df):\n",
        "    el[\"FS_triples\"] = el[\"FS_triples\"].split(\"Extracted Triples:\\n\")[-1].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOHpdGpzRR2_",
        "outputId": "fd71e69d-4c3a-4ee0-9c0f-9218a9a6cb7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Li Ying) - [PLAYED_IN] -> (France 2019)\n",
            "(China PR) - [PARTICIPATED_IN] -> (France 2019)\n",
            "(Argentina) - [PARTICIPATED_IN] -> (France 2019)\n",
            "(Australia) - [PARTICIPATED_IN] -> (France 2019)\n",
            "(Brazil) - [PARTICIPATED_IN] -> (France 2019)\n",
            "(Cameroon) - [PARTICIPATED_IN] -> (France 2019)\n",
            "(Canada) - [PARTICIPATED_IN] -> (France 2019)\n",
            "(Chile) - [PARTICIPATED_IN] -> (France 2019)</s>\n"
          ]
        }
      ],
      "source": [
        "print(formatted_test_df[0][\"FS_triples\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yIeVPoTRR2_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "json_str = json.dumps(formatted_test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2YAT4dsRR2_"
      },
      "outputs": [],
      "source": [
        "with open('FSresponses2.json', 'w') as f:\n",
        "    json.dump(json_str, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZeEId62RR2_"
      },
      "source": [
        "# Generate responses (finetuned model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHOpW7NQRR2_"
      },
      "outputs": [],
      "source": [
        "# importing the module\n",
        "import json\n",
        "\n",
        "# Opening JSON file\n",
        "with open('FSresponses.json') as json_file:\n",
        "    data = json.load(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJY-iRSxRR3A"
      },
      "outputs": [],
      "source": [
        "formatted_test_df = json.loads(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ibc-Xt0RR3A",
        "outputId": "8a747e64-1ebe-40fa-8ecb-6f51e728d536"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST]Imagine being a Knowledge Graph constructor from unstructured text.\n",
            "Following the schema provided, extract all the triples you can find in the text.\n",
            "Make sure to mantain the formatting of the schema: \"(subject)-[PREDICATE]->(Object)\"\n",
            "\n",
            "Schema:\n",
            "(Team)-[PARTICIPATED_IN]->(Tournament)\n",
            "(Person)-[PLAYED_IN]->(Tournament)\n",
            "(Person)-[REPRESENTS]->(Team)\n",
            "\n",
            "Context:\n",
            "\"France 2019 was a soccer tournament held in France, which saw participation from various teams around the world. The China PR team, represented by Li Ying, participated in the tournament. Additionally, Argentina, Australia, Brazil, Cameroon, Canada, and Chile also took part in the event.\"[/INST]\n",
            "\n",
            "Extracted Triples:\n"
          ]
        }
      ],
      "source": [
        "print(formatted_test_df[0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtybicPiRR3A"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "ft_model = PeftModel.from_pretrained(base_model, \"Prova1/checkpoint-1000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "bfz3lXsSRR3A",
        "outputId": "98d664c7-9b4c-4fcb-c8a8-8f181d25ffa3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32000, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralSdpaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): MistralRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=14336, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "      (lm_head): lora.Linear(\n",
              "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "        (lora_dropout): ModuleDict(\n",
              "          (default): Dropout(p=0.05, inplace=False)\n",
              "        )\n",
              "        (lora_A): ModuleDict(\n",
              "          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "        )\n",
              "        (lora_B): ModuleDict(\n",
              "          (default): Linear(in_features=8, out_features=32000, bias=False)\n",
              "        )\n",
              "        (lora_embedding_A): ParameterDict()\n",
              "        (lora_embedding_B): ParameterDict()\n",
              "        (lora_magnitude_vector): ModuleDict()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    model_max_length=1024,\n",
        "    padding_side=\"left\")\n",
        "\n",
        "ft_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9g2VqBDRR3E"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.unk_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00CHKpVNRR3E"
      },
      "outputs": [],
      "source": [
        "def generate_response(eval_prompt):\n",
        "    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        response = tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=512, repetition_penalty=1.15, pad_token_id=tokenizer.unk_token_id)[0], skip_special_tokens=False)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "g9DOFVi9RR3E",
        "outputId": "6b0922c1-3db0-4a48-d569-dbc277cbf73d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [47:36<00:00, 14.28s/it]\n"
          ]
        }
      ],
      "source": [
        "for el in tqdm(formatted_test_df):\n",
        "    text = el[\"text\"].replace('Make sure to mantain the formatting of the schema: \"(subject)-[PREDICATE]->(Object)\"\\n', '')\n",
        "    response = generate_response(text.strip())\n",
        "    el[\"FT_triples\"] = response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9VGDyvnRR3E",
        "outputId": "eaaf38e7-1717-49c6-c0a2-e24221802ffc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:00<00:00, 199254.35it/s]\n"
          ]
        }
      ],
      "source": [
        "for el in tqdm(formatted_test_df):\n",
        "    el[\"FT_triples\"] = el[\"FT_triples\"].split(\"Extracted Triples:\\n\")[-1].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pTe3BJVRR3F"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "json_str = json.dumps(formatted_test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA-gvXDmRR3F"
      },
      "outputs": [],
      "source": [
        "with open('responses_external_dbs.json', 'w') as f:\n",
        "    json.dump(json_str, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Fine-tuned model with Few Shot prompting\n",
        "We use the first fine-tuned model (the one finetuned without examples in its observations). We give it in input the same observations we used for the Base Model with Few Shot prompting."
      ],
      "metadata": {
        "id": "YDvBlPCoSDUJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "gBvb8wDZR_UD",
        "outputId": "3fb05a6d-d630-44c9-e6be-5e7f03da90fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/200 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2700 > 1024). Running this sequence through the model will result in indexing errors\n",
            "100%|██████████| 200/200 [51:55<00:00, 15.58s/it] \n"
          ]
        }
      ],
      "source": [
        "for el in tqdm(formatted_test_df):\n",
        "    text = el[\"FS_prompt\"].replace('Make sure to mantain the formatting of the schema: \"(subject)-[PREDICATE]->(Object)\"\\n', '')\n",
        "    response = generate_response(text.strip())\n",
        "    el[\"FT_FS_triples\"] = response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZaG6nPyR_UD",
        "outputId": "c480a056-38b7-4a49-9524-15b9950665ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:00<00:00, 107808.87it/s]\n"
          ]
        }
      ],
      "source": [
        "for el in tqdm(formatted_test_df):\n",
        "    el[\"FT_FS_triples\"] = el[\"FT_FS_triples\"].split(\"Extracted Triples:\\n\")[-1].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYuDmo_bSSWa"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "json_str = json.dumps(formatted_test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2B_73j7SSWa"
      },
      "outputs": [],
      "source": [
        "with open('responses_external_dbs.json', 'w') as f:\n",
        "    json.dump(json_str, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second Fine-tuned model\n",
        "Now we use the second fine-tuned model (the one finetuned WITH examples in its observations)."
      ],
      "metadata": {
        "id": "-Wt2e63SStlu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoint 50"
      ],
      "metadata": {
        "id": "fxN8d6BqTxRE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZPrFNzNTnTr"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "ft_model = PeftModel.from_pretrained(base_model, \"Prova_FS/checkpoint-50\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "outputId": "7c586a6e-a380-42e7-e231-9031f351f562",
        "id": "7OtYgR17TnT1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32000, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): MistralRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=14336, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "      (lm_head): lora.Linear(\n",
              "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "        (lora_dropout): ModuleDict(\n",
              "          (default): Dropout(p=0.05, inplace=False)\n",
              "        )\n",
              "        (lora_A): ModuleDict(\n",
              "          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "        )\n",
              "        (lora_B): ModuleDict(\n",
              "          (default): Linear(in_features=8, out_features=32000, bias=False)\n",
              "        )\n",
              "        (lora_embedding_A): ParameterDict()\n",
              "        (lora_embedding_B): ParameterDict()\n",
              "        (lora_magnitude_vector): ModuleDict()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    model_max_length=4096,\n",
        "    padding_side=\"left\")\n",
        "\n",
        "ft_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCa_ZwogTnT2"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.unk_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D78rBvdRTnT2"
      },
      "outputs": [],
      "source": [
        "def generate_response(eval_prompt):\n",
        "    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        response = tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=512, repetition_penalty=1.15, pad_token_id=tokenizer.unk_token_id)[0], skip_special_tokens=False)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "outputId": "f3c0f23c-5a77-4f89-9d3c-24face8f53b4",
        "id": "rO9kvfs5TnT2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [1:12:13<00:00, 21.67s/it]\n"
          ]
        }
      ],
      "source": [
        "for el in tqdm(formatted_test_df):\n",
        "    text = el[\"FS_prompt\"].replace('Make sure to mantain the formatting of the schema: \"(subject)-[PREDICATE]->(Object)\"\\n', '')\n",
        "    response = generate_response(text.strip())\n",
        "    el[\"FSFT_triples_50\"] = response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "cb877ac5-8e5e-48cf-a393-a3c9f0634fb6",
        "id": "xo4poEx-YZ7P"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:00<00:00, 124367.80it/s]\n"
          ]
        }
      ],
      "source": [
        "for el in tqdm(formatted_test_df):\n",
        "    el[\"FSFT_triples_50\"] = el[\"FSFT_triples_50\"].split(\"Extracted Triples:\\n\")[-1].strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoint 150"
      ],
      "metadata": {
        "id": "V6C6yYVgT4DL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PE9cc8OCR_UG"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "ft_model = PeftModel.from_pretrained(base_model, \"Prova_FS/checkpoint-150\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "20OQFRrhR_UG",
        "outputId": "f0d8a6ac-6918-47fb-d050-c02d3501d932"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32000, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): MistralRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=14336, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "      (lm_head): lora.Linear(\n",
              "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "        (lora_dropout): ModuleDict(\n",
              "          (default): Dropout(p=0.05, inplace=False)\n",
              "        )\n",
              "        (lora_A): ModuleDict(\n",
              "          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "        )\n",
              "        (lora_B): ModuleDict(\n",
              "          (default): Linear(in_features=8, out_features=32000, bias=False)\n",
              "        )\n",
              "        (lora_embedding_A): ParameterDict()\n",
              "        (lora_embedding_B): ParameterDict()\n",
              "        (lora_magnitude_vector): ModuleDict()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    model_max_length=4096,\n",
        "    padding_side=\"left\")\n",
        "\n",
        "ft_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQcAVzizR_UH"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.unk_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vj5kEmCAR_UH"
      },
      "outputs": [],
      "source": [
        "def generate_response(eval_prompt):\n",
        "    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        response = tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=512, repetition_penalty=1.15, pad_token_id=tokenizer.unk_token_id)[0], skip_special_tokens=False)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "qvgGFs_PR_UH",
        "outputId": "f1b45805-3436-4e94-a6ac-b606f2c7a77e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/200 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
            "100%|██████████| 200/200 [1:01:40<00:00, 18.50s/it]\n"
          ]
        }
      ],
      "source": [
        "for el in tqdm(formatted_test_df):\n",
        "    text = el[\"FS_prompt\"].replace('Make sure to mantain the formatting of the schema: \"(subject)-[PREDICATE]->(Object)\"\\n', '')\n",
        "    response = generate_response(text.strip())\n",
        "    el[\"FSFT_triples_150\"] = response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCfDCUwdR_UI",
        "outputId": "cb877ac5-8e5e-48cf-a393-a3c9f0634fb6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:00<00:00, 124367.80it/s]\n"
          ]
        }
      ],
      "source": [
        "for el in tqdm(formatted_test_df):\n",
        "    el[\"FSFT_triples_150\"] = el[\"FSFT_triples_150\"].split(\"Extracted Triples:\\n\")[-1].strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z9-NPd3tTPJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoint 300"
      ],
      "metadata": {
        "id": "vO9tYrNPT04x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLcFLMzKR_UK"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "ft_model = PeftModel.from_pretrained(base_model, \"Prova_FS/checkpoint-300\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "fKjz3n0TR_UK",
        "outputId": "27d64d0c-dff4-4001-e421-b5b07eec86e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32000, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): MistralRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=14336, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "      (lm_head): lora.Linear(\n",
              "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "        (lora_dropout): ModuleDict(\n",
              "          (default): Dropout(p=0.05, inplace=False)\n",
              "        )\n",
              "        (lora_A): ModuleDict(\n",
              "          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "        )\n",
              "        (lora_B): ModuleDict(\n",
              "          (default): Linear(in_features=8, out_features=32000, bias=False)\n",
              "        )\n",
              "        (lora_embedding_A): ParameterDict()\n",
              "        (lora_embedding_B): ParameterDict()\n",
              "        (lora_magnitude_vector): ModuleDict()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    model_max_length=4096,\n",
        "    padding_side=\"left\")\n",
        "\n",
        "ft_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ7ePkAuR_UL"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.unk_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1CW-pVwR_UL"
      },
      "outputs": [],
      "source": [
        "def generate_response(eval_prompt):\n",
        "    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        response = tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=512, repetition_penalty=1.15, pad_token_id=tokenizer.unk_token_id)[0], skip_special_tokens=False)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "NokDe9xdR_UL",
        "outputId": "30621abf-611c-40c8-c008-d942e31d3ee8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [1:03:53<00:00, 19.17s/it]\n"
          ]
        }
      ],
      "source": [
        "for el in tqdm(formatted_test_df):\n",
        "    text = el[\"FS_prompt\"].replace('Make sure to mantain the formatting of the schema: \"(subject)-[PREDICATE]->(Object)\"\\n', '')\n",
        "    response = generate_response(text.strip())\n",
        "    el[\"FSFT_triples_300\"] = response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK-JDIadR_UL",
        "outputId": "e1eb2f17-a94d-4600-e097-bc66af0a0787"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:00<00:00, 34262.99it/s]\n"
          ]
        }
      ],
      "source": [
        "for el in tqdm(formatted_test_df):\n",
        "    el[\"FSFT_triples_300\"] = el[\"FSFT_triples_300\"].split(\"Extracted Triples:\\n\")[-1].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpLI8etJR_UP"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "json_str = json.dumps(formatted_test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXPmiiRYR_UP"
      },
      "outputs": [],
      "source": [
        "with open('responses_external_dbs.json', 'w') as f:\n",
        "    json.dump(json_str, f)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}